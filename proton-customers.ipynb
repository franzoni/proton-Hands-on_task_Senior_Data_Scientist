{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> picke already exists, much faster using it than opening excel files\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# DATASET FROM: http://archive.ics.uci.edu/ml/datasets/Online+Retail+II\n",
    "\n",
    "# GF TODO: check if excel present, otherwise download it\n",
    "# wget http://archive.ics.uci.edu/ml/machine-learning-databases/00502/online_retail_II.xlsx\n",
    "\n",
    "excel_with_path  = '/eos/user/f/franzoni/SWAN_projects/proton/online_retail_II.xlsx'\n",
    "pickle_with_path = '/eos/user/f/franzoni/SWAN_projects/proton/online_retail_II.pkl'\n",
    "csv_with_path = '/eos/user/f/franzoni/SWAN_projects/proton/online_retail_II.csv'\n",
    "\n",
    "df = None\n",
    "\n",
    "if os.path.isfile(pickle_with_path):\n",
    "    print('-> picke already exists, much faster using it than opening excel files\\n')\n",
    "    df = pd.read_pickle(pickle_with_path)\n",
    "else:\n",
    "    print('-> picke does not exist, go to excel, and create it\\n')\n",
    "    df1 = pd.read_excel (excel_with_path,'Year 2009-2010')\n",
    "    df2 = pd.read_excel (excel_with_path,'Year 2010-2011')\n",
    "    df = pd.concat([df1, df2])\n",
    "    df.to_pickle(pickle_with_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define transaction: total cash for a single 'row' of the dataset, i.e. item_price times the number of items bought\n",
    "df['Transaction'] = df.Quantity * df.Price\n",
    "df=df.rename(columns={\"Customer ID\": \"Customer_ID\"})\n",
    "\n",
    "# day, week and time are added for convenience of analysis later on\n",
    "from datetime import datetime\n",
    "df['InvoiceDay'] = df['InvoiceDate'].map(lambda p: p.date())\n",
    "df['InvoiceWeekDay'] = df['InvoiceDate'].map(lambda p: p.weekday())\n",
    "df['InvoiceTime'] = df['InvoiceDate'].map(lambda p: p.time())\n",
    "df['InvoiceWeek'] = df['InvoiceDate'].map(lambda p: p.isocalendar()[1]+52*(p.year-2010))\n",
    "\n",
    "# avoide negative weeks and start counting from the first week of the dataset, which starts from 01/12/2009\n",
    "df['InvoiceWeek'] = df['InvoiceWeek']+3\n",
    "\n",
    "\n",
    "# implement the definition of cancellation in the documentation:\n",
    "# http://archive.ics.uci.edu/ml/datasets/Online+Retail+II\n",
    "#      ==> \"If this code starts with the letter 'c', it indicates a cancellation.\"\"\n",
    "def is_cancellation(s):\n",
    "    if isinstance(s, int):\n",
    "        #print \"ordinary invoice\"\n",
    "        return 0\n",
    "    elif isinstance(s, unicode):\n",
    "        #print \"unicode string\"\n",
    "        if s.rfind('C')!=-1:\n",
    "            return 1\n",
    "        else:\n",
    "            # print \"Something unexpected\"  # found, e.g. A506401\n",
    "            # print s\n",
    "            return 2\n",
    "df['IsCancellation'] = df['Invoice'].map(is_cancellation)     \n",
    "      \n",
    "\n",
    "\n",
    "# items with prices above ~1500 are so few that is worth looking at them in detail, and excluding them from the plots\n",
    "# There'a lot of transactions with price set to 0, which based on the descriptions are \n",
    "max_item_price=1400\n",
    "r             =(0,max_item_price)\n",
    "def is_ordinary_item(p):\n",
    "    if abs(p)>max_item_price or p==0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "df['IsOrdinaryItem'] = df['Price'].map(is_ordinary_item)        \n",
    "\n",
    "# df.to_csv(csv_with_path, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nulls</th>\n",
       "      <th>rel. nulls [%]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Customer_ID</th>\n",
       "      <td>243007</td>\n",
       "      <td>22.766873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Description</th>\n",
       "      <td>4382</td>\n",
       "      <td>0.410541</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              nulls  rel. nulls [%]\n",
       "Customer_ID  243007       22.766873\n",
       "Description    4382        0.410541"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what are the nulls in each feature?\n",
    "nulls = df.isnull().sum()[df.isnull().sum() != 0]\n",
    "\n",
    "# select all transactions containing \n",
    "df_nulls = df[df.isnull().any(axis=1)].copy(deep=True)\n",
    "\n",
    "nulls_rel = nulls/df.shape[0]*100\n",
    "\n",
    "nulls_summary = pd.concat([nulls, nulls_rel], axis=1, keys=['nulls', 'rel. nulls [%]'])\\\n",
    "               .sort_values('nulls', ascending=False)\n",
    "\n",
    "nulls_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#<a id='the_destination'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## todo\n",
    "- nan and empty -> cleanup   # IN THE MAKING -> only consequences on the users study\n",
    "- add colums for data and time, separately ?    # DONE\n",
    "- create a second DF by user: RFM added to\n",
    "\n",
    "\n",
    "## What do I want to extract from this dataset?\n",
    "- https://en.wikipedia.org/wiki/Exploratory_data_analysis\n",
    "\n",
    "- how many customers   DONE, also as a function of country\n",
    "- make a pliot of transaction, of purchase prices, and of invoice_price! #  DONE\n",
    "\n",
    "- VS TIME: trends of spending overall, by country\n",
    "- revenue as a function of date, as a function of the time in the day  # DONE\n",
    "- trends of spending: overall, by the largest customer, by the smaller customers (TOO SPECIFIC?)\n",
    "\n",
    "- NEED TO BUILD A PER CUSTOMER DF\n",
    "- customer: how many transactions, how much total revenue:\n",
    "    => BREAK DOWN BY country, tra\n",
    "- RMF\n",
    "\n",
    "\n",
    "- ==> do this\n",
    "- how many different types of items                  # DONE\n",
    "- what kind of items are bought the most (by NUMBER of by REVENUE), are cancelled the most, \n",
    "- how much revenue per type of item bought\n",
    "\n",
    "- correlation between\n",
    "- cancellations: fraction of cancellation by nunber of transactions and by proportion of renenue\n",
    "-                correlation to CHURNING ? Correlation to country OR type of good purchased ?\n",
    "\n",
    "- cust.groupby('customer_unique_id').size().value_counts() => The majority of customers made only a single purchase. # DONE\n",
    "\n",
    "\n",
    "- Can I cathegorise the purchasable items? => If so, customers split across those cathegories\n",
    "- Persona; how many cheap items, few expensive ones?\n",
    "- https://cxl.com/blog/creating-customer-personas-using-data-driven-research/\n",
    "- now many items are bought per session, how much is spent per session\n",
    "\n",
    "==> TOWARDS CUSTOMER PERSONA DEFINITION\n",
    "- MANY TRANSACTIONS ARE RECORDED AT THE SAME TIME => as if they were a single shopping session, but billed in split goups of goods\n",
    "- the function summary_data_from_transaction_data treats transactions taking place on the same day as A SINGLE ONE!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"section_ID_GF\"></a>\n",
    "\n",
    "- this section can be reached at this URL https://swan001.cern.ch/user/franzoni/notebooks/SWAN_projects/proton/proton-EDA.ipynb#section_ID_GF\n",
    "- thanks to this documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_customers\"></a>\n",
    "## Customers carachteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plan for customers\n",
    "- make a dataframe\n",
    "- add also the RFM variables\n",
    "- add nuber of items bought in total, number of different TYPEs of items\n",
    "- add total value of course\n",
    "- Can I cathegorise the purchasable items? => If so, customers split across those cathegories\n",
    "- Persona; how many cheap items, few expensive ones?\n",
    "- https://cxl.com/blog/creating-customer-personas-using-data-driven-research/\n",
    "- now many items are bought per session, how much is spent per session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1       146\n",
       "6        84\n",
       "11       82\n",
       "16       80\n",
       "12       79\n",
       "18       77\n",
       "13       76\n",
       "5        75\n",
       "20       74\n",
       "21       74\n",
       "7        73\n",
       "8        72\n",
       "9        71\n",
       "14       70\n",
       "19       67\n",
       "2        65\n",
       "10       64\n",
       "23       64\n",
       "17       60\n",
       "4        60\n",
       "35       58\n",
       "15       58\n",
       "29       57\n",
       "28       56\n",
       "22       55\n",
       "25       54\n",
       "26       54\n",
       "27       53\n",
       "30       53\n",
       "34       52\n",
       "       ... \n",
       "658       1\n",
       "662       1\n",
       "694       1\n",
       "718       1\n",
       "730       1\n",
       "750       1\n",
       "478       1\n",
       "462       1\n",
       "1049      1\n",
       "446       1\n",
       "1069      1\n",
       "1089      1\n",
       "1109      1\n",
       "1133      1\n",
       "1241      1\n",
       "1301      1\n",
       "1353      1\n",
       "1419      1\n",
       "178       1\n",
       "234       1\n",
       "282       1\n",
       "330       1\n",
       "362       1\n",
       "390       1\n",
       "394       1\n",
       "402       1\n",
       "406       1\n",
       "422       1\n",
       "434       1\n",
       "1885      1\n",
       "Length: 701, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('Customer_ID').size().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='the_destination'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Pippo goes to the cinema](section-title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
