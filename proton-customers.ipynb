{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# DATASET FROM: http://archive.ics.uci.edu/ml/datasets/Online+Retail+II\n",
    "\n",
    "# GF TODO: check if excel present, otherwise download it\n",
    "# wget http://archive.ics.uci.edu/ml/machine-learning-databases/00502/online_retail_II.xlsx\n",
    "\n",
    "excel_with_path  = '/eos/user/f/franzoni/SWAN_projects/proton/online_retail_II.xlsx'\n",
    "pickle_with_path = '/eos/user/f/franzoni/SWAN_projects/proton/online_retail_II.pkl'\n",
    "csv_with_path = '/eos/user/f/franzoni/SWAN_projects/proton/online_retail_II.csv'\n",
    "\n",
    "df = None\n",
    "\n",
    "if os.path.isfile(pickle_with_path):\n",
    "    print('-> picke already exists, much faster using it than opening excel files\\n')\n",
    "    df = pd.read_pickle(pickle_with_path)\n",
    "else:\n",
    "    print('-> picke does not exist, go to excel, and create it\\n')\n",
    "    df1 = pd.read_excel (excel_with_path,'Year 2009-2010')\n",
    "    df2 = pd.read_excel (excel_with_path,'Year 2010-2011')\n",
    "    df = pd.concat([df1, df2])\n",
    "    df.to_pickle(pickle_with_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define transaction: total cash for a single 'row' of the dataset, i.e. item_price times the number of items bought\n",
    "df['Transaction'] = df.Quantity * df.Price\n",
    "df=df.rename(columns={\"Customer ID\": \"Customer_ID\"})\n",
    "\n",
    "# day, week and time are added for convenience of analysis later on\n",
    "from datetime import datetime\n",
    "df['InvoiceDay'] = df['InvoiceDate'].map(lambda p: p.date())\n",
    "df['InvoiceWeekDay'] = df['InvoiceDate'].map(lambda p: p.weekday())\n",
    "df['InvoiceTime'] = df['InvoiceDate'].map(lambda p: p.time())\n",
    "df['InvoiceWeek'] = df['InvoiceDate'].map(lambda p: p.isocalendar()[1]+52*(p.year-2010))\n",
    "\n",
    "# avoide negative weeks and start counting from the first week of the dataset, which starts from 01/12/2009\n",
    "df['InvoiceWeek'] = df['InvoiceWeek']+3\n",
    "\n",
    "\n",
    "# implement the definition of cancellation in the documentation:\n",
    "# http://archive.ics.uci.edu/ml/datasets/Online+Retail+II\n",
    "#      ==> \"If this code starts with the letter 'c', it indicates a cancellation.\"\"\n",
    "def is_cancellation(s):\n",
    "    if isinstance(s, int):\n",
    "        #print \"ordinary invoice\"\n",
    "        return 0\n",
    "    elif isinstance(s, unicode):\n",
    "        #print \"unicode string\"\n",
    "        if s.rfind('C')!=-1:\n",
    "            return 1\n",
    "        else:\n",
    "            # print \"Something unexpected\"  # found, e.g. A506401\n",
    "            # print s\n",
    "            return 2\n",
    "df['IsCancellation'] = df['Invoice'].map(is_cancellation)     \n",
    "      \n",
    "\n",
    "\n",
    "# items with prices above ~1500 are so few that is worth looking at them in detail, and excluding them from the plots\n",
    "# There'a lot of transactions with price set to 0, which based on the descriptions are \n",
    "max_item_price=1400\n",
    "r             =(0,max_item_price)\n",
    "def is_ordinary_item(p):\n",
    "    if abs(p)>max_item_price or p==0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "df['IsOrdinaryItem'] = df['Price'].map(is_ordinary_item)        \n",
    "\n",
    "# df.to_csv(csv_with_path, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are the nulls in each feature?\n",
    "nulls = df.isnull().sum()[df.isnull().sum() != 0]\n",
    "\n",
    "# select all transactions containing \n",
    "df_nulls = df[df.isnull().any(axis=1)].copy(deep=True)\n",
    "\n",
    "nulls_rel = nulls/df.shape[0]*100\n",
    "\n",
    "nulls_summary = pd.concat([nulls, nulls_rel], axis=1, keys=['nulls', 'rel. nulls [%]'])\\\n",
    "               .sort_values('nulls', ascending=False)\n",
    "\n",
    "nulls_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#<a id='the_destination'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## todo\n",
    "- nan and empty -> cleanup   # IN THE MAKING -> only consequences on the users study\n",
    "- add colums for data and time, separately ?    # DONE\n",
    "- create a second DF by user: RFM added to\n",
    "\n",
    "\n",
    "## What do I want to extract from this dataset?\n",
    "- https://en.wikipedia.org/wiki/Exploratory_data_analysis\n",
    "\n",
    "- how many customers   DONE, also as a function of country\n",
    "- make a pliot of transaction, of purchase prices, and of invoice_price! #  DONE\n",
    "\n",
    "- VS TIME: trends of spending overall, by country\n",
    "- revenue as a function of date, as a function of the time in the day  # DONE\n",
    "- trends of spending: overall, by the largest customer, by the smaller customers (TOO SPECIFIC?)\n",
    "\n",
    "- NEED TO BUILD A PER CUSTOMER DF\n",
    "- customer: how many transactions, how much total revenue:\n",
    "    => BREAK DOWN BY country, tra\n",
    "- RMF\n",
    "\n",
    "\n",
    "- ==> do this\n",
    "- how many different types of items                  # DONE\n",
    "- what kind of items are bought the most (by NUMBER of by REVENUE), are cancelled the most, \n",
    "- how much revenue per type of item bought\n",
    "\n",
    "- correlation between\n",
    "- cancellations: fraction of cancellation by nunber of transactions and by proportion of renenue\n",
    "-                correlation to CHURNING ? Correlation to country OR type of good purchased ?\n",
    "\n",
    "- cust.groupby('customer_unique_id').size().value_counts() => The majority of customers made only a single purchase. # DONE\n",
    "\n",
    "\n",
    "- Can I cathegorise the purchasable items? => If so, customers split across those cathegories\n",
    "- Persona; how many cheap items, few expensive ones?\n",
    "- https://cxl.com/blog/creating-customer-personas-using-data-driven-research/\n",
    "- now many items are bought per session, how much is spent per session\n",
    "\n",
    "==> TOWARDS CUSTOMER PERSONA DEFINITION\n",
    "- MANY TRANSACTIONS ARE RECORDED AT THE SAME TIME => as if they were a single shopping session, but billed in split goups of goods\n",
    "- the function summary_data_from_transaction_data treats transactions taking place on the same day as A SINGLE ONE!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"section_ID_GF\"></a>\n",
    "\n",
    "- this section can be reached at this URL https://swan001.cern.ch/user/franzoni/notebooks/SWAN_projects/proton/proton-EDA.ipynb#section_ID_GF\n",
    "- thanks to this documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_customers\"></a>\n",
    "## Customers carachteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plan for customers\n",
    "- make a dataframe\n",
    "- add also the RFM variables\n",
    "- add nuber of items bought in total, number of different TYPEs of items\n",
    "- add total value of course\n",
    "- Can I cathegorise the purchasable items? => If so, customers split across those cathegories\n",
    "- Persona; how many cheap items, few expensive ones?\n",
    "- https://cxl.com/blog/creating-customer-personas-using-data-driven-research/\n",
    "\n",
    "- DF to containg:\n",
    "- filter out non standard AND null customers SHALL I REMOVE CANCELLATIONS ????? => yes, keep it basic for now!\n",
    "- RFM\n",
    "- PER TOTAL:    number of items per customer, number of DIFFERENT items per customer\n",
    "- PER INVOICE:  same-as-above\n",
    "- What's the number of orders per customer?Â¶\n",
    "- number of items per purchase, amount per purchase - of each different customer (do I alraedy have this ?)\n",
    "\n",
    "+ RFM ANALYSIS:\n",
    "- ==> add to the dataframe the quantiles of RFM, with the correct ordering/SCORE\n",
    "- https://joaocorreia.io/blog/rfm-analysis-increase-sales-by-segmenting-your-customers.html\n",
    "- https://www.blastanalytics.com/blog/rfm-analysis-boosts-sales\n",
    "\n",
    "+ CLUSTERING\n",
    "- https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set#The_silhouette_method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='the_destination'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Pippo goes to the cinema](section-title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup of data for customer analysis\n",
    "- we decide to remove all datata points with no Customer_ID\n",
    "- and to limit the study only to IsOrdinaryItem (as defined above), to avoid skewing the resuts w/ dataa points which are not actual purchases\n",
    "- to keep the modelling basic, for now we decide also to remove all cancellation oders\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
