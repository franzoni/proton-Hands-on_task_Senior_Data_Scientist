{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# DATASET FROM: http://archive.ics.uci.edu/ml/datasets/Online+Retail+II\n",
    "\n",
    "# GF TODO: check if excel present, otherwise download it\n",
    "# wget http://archive.ics.uci.edu/ml/machine-learning-databases/00502/online_retail_II.xlsx\n",
    "\n",
    "excel_with_path  = '/eos/user/f/franzoni/SWAN_projects/proton/online_retail_II.xlsx'\n",
    "pickle_with_path = '/eos/user/f/franzoni/SWAN_projects/proton/online_retail_II.pkl'\n",
    "csv_with_path = '/eos/user/f/franzoni/SWAN_projects/proton/online_retail_II.csv'\n",
    "\n",
    "df = None\n",
    "\n",
    "if os.path.isfile(pickle_with_path):\n",
    "    print('-> picke already exists, much faster using it than opening excel files\\n')\n",
    "    df = pd.read_pickle(pickle_with_path)\n",
    "else:\n",
    "    print('-> picke does not exist, go to excel, and create it\\n')\n",
    "    df1 = pd.read_excel (excel_with_path,'Year 2009-2010')\n",
    "    df2 = pd.read_excel (excel_with_path,'Year 2010-2011')\n",
    "    df = pd.concat([df1, df2])\n",
    "    df.to_pickle(pickle_with_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define transaction: total cash for a single 'row' of the dataset, i.e. item_price times the number of items bought\n",
    "df['Transaction'] = df.Quantity * df.Price\n",
    "df=df.rename(columns={\"Customer ID\": \"Customer_ID\"})\n",
    "\n",
    "# day, week and time are added for convenience of analysis later on\n",
    "from datetime import datetime\n",
    "df['InvoiceDay'] = df['InvoiceDate'].map(lambda p: p.date())\n",
    "df['InvoiceWeekDay'] = df['InvoiceDate'].map(lambda p: p.weekday())\n",
    "df['InvoiceTime'] = df['InvoiceDate'].map(lambda p: p.time())\n",
    "df['InvoiceWeek'] = df['InvoiceDate'].map(lambda p: p.isocalendar()[1]+52*(p.year-2010))\n",
    "\n",
    "# avoide negative weeks and start counting from the first week of the dataset, which starts from 01/12/2009\n",
    "df['InvoiceWeek'] = df['InvoiceWeek']+3\n",
    "\n",
    "\n",
    "# implement the definition of cancellation in the documentation:\n",
    "# http://archive.ics.uci.edu/ml/datasets/Online+Retail+II\n",
    "#      ==> \"If this code starts with the letter 'c', it indicates a cancellation.\"\"\n",
    "def is_cancellation(s):\n",
    "    if isinstance(s, int):\n",
    "        #print \"ordinary invoice\"\n",
    "        return 0\n",
    "    elif isinstance(s, unicode):\n",
    "        #print \"unicode string\"\n",
    "        if s.rfind('C')!=-1:\n",
    "            return 1\n",
    "        else:\n",
    "            # print \"Something unexpected\"  # found, e.g. A506401\n",
    "            # print s\n",
    "            return 2\n",
    "df['IsCancellation'] = df['Invoice'].map(is_cancellation)     \n",
    "      \n",
    "\n",
    "\n",
    "# items with prices above ~1500 are so few that is worth looking at them in detail, and excluding them from the plots\n",
    "# There'a lot of transactions with price set to 0, which based on the descriptions are \n",
    "max_item_price=1400\n",
    "r             =(0,max_item_price)\n",
    "def is_ordinary_item(p):\n",
    "    if abs(p)>max_item_price or p==0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "df['IsOrdinaryItem'] = df['Price'].map(is_ordinary_item)        \n",
    "\n",
    "# df.to_csv(csv_with_path, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are the nulls in each feature?\n",
    "nulls = df.isnull().sum()[df.isnull().sum() != 0]\n",
    "\n",
    "# select all transactions containing \n",
    "df_nulls = df[df.isnull().any(axis=1)].copy(deep=True)\n",
    "\n",
    "nulls_rel = nulls/df.shape[0]*100\n",
    "\n",
    "nulls_summary = pd.concat([nulls, nulls_rel], axis=1, keys=['nulls', 'rel. nulls [%]'])\\\n",
    "               .sort_values('nulls', ascending=False)\n",
    "\n",
    "nulls_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEARLY all cancellations have either Quantity or Price set to negative values\n",
    "# there are only two exceptions, here below:\n",
    "#           one of which is an actual cancellation\n",
    "#           the second of which is an onforeseen transaction type which starts with A\n",
    "(df.loc[ (df.IsCancellation > 0.) & (df.Transaction > 0.) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are 6 data entries which are neither normal transactions nor cancellations which are neither \n",
    "# they're described as \"Adjust bad debt\"\n",
    "(df.loc[ (df.IsCancellation == 2) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the \"Adjust bad debt\"\n",
    "df = (df.loc[ (df.IsCancellation < 2) ])\n",
    "\n",
    "#from IPython.core.display import display, HTML\n",
    "#display(HTML(\"\"\"<a href=\"https://google.at\">text</a>\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#<a id='the_destination'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## todo\n",
    "- nan and empty -> cleanup   # IN THE MAKING -> only consequences on the users study\n",
    "- add colums for data and time, separately ?    # DONE\n",
    "- create a second DF by user: RFM added to\n",
    "\n",
    "\n",
    "## What do I want to extract from this dataset?\n",
    "- https://en.wikipedia.org/wiki/Exploratory_data_analysis\n",
    "\n",
    "- how many customers   DONE, also as a function of country\n",
    "- make a pliot of transaction, of purchase prices, and of invoice_price! #  DONE\n",
    "\n",
    "- VS TIME: trends of spending overall, by country\n",
    "- revenue as a function of date, as a function of the time in the day  # DONE\n",
    "- trends of spending: overall, by the largest customer, by the smaller customers (TOO SPECIFIC?)\n",
    "\n",
    "- NEED TO BUILD A PER CUSTOMER DF\n",
    "- customer: how many transactions, how much total revenue:\n",
    "    => BREAK DOWN BY country, tra\n",
    "- RMF\n",
    "\n",
    "\n",
    "- ==> do this\n",
    "- how many different types of items                  # DONE\n",
    "- what kind of items are bought the most (by NUMBER of by REVENUE), are cancelled the most, \n",
    "- how much revenue per type of item bought\n",
    "\n",
    "- correlation between\n",
    "- cancellations: fraction of cancellation by nunber of transactions and by proportion of renenue\n",
    "-                correlation to CHURNING ? Correlation to country OR type of good purchased ?\n",
    "\n",
    "- cust.groupby('customer_unique_id').size().value_counts() => The majority of customers made only a single purchase. # DONE\n",
    "\n",
    "\n",
    "- Can I cathegorise the purchasable items? => If so, customers split across those cathegories\n",
    "- Persona; how many cheap items, few expensive ones?\n",
    "- https://cxl.com/blog/creating-customer-personas-using-data-driven-research/\n",
    "- now many items are bought per session, how much is spent per session\n",
    "\n",
    "==> TOWARDS CUSTOMER PERSONA DEFINITION\n",
    "- MANY TRANSACTIONS ARE RECORDED AT THE SAME TIME => as if they were a single shopping session, but billed in split goups of goods\n",
    "- the function summary_data_from_transaction_data treats transactions taking place on the same day as A SINGLE ONE!\n",
    "\n",
    "- What are the most popular products that are bought in the UK?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"section_ID_GF\"></a>\n",
    "\n",
    "- this section can be reached at this URL https://swan001.cern.ch/user/franzoni/notebooks/SWAN_projects/proton/proton-EDA.ipynb#section_ID_GF\n",
    "- thanks to this documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_single_items\"></a>\n",
    "## SINGLE ITEM PRICE AND single item purchases\n",
    "- how many single items are there (ordinary transaction) # DONE\n",
    "- what is the cost of each single item\n",
    "- what is the purchase frequency of each item\n",
    "\n",
    "- what is the resulting price distribution for all transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I consider here only transactions with positive proceeds\n",
    "\n",
    "transaction_plus   = (df.loc[df.Transaction > 0.]).Transaction.agg([sum])\n",
    "transaction_cancel = (df.loc[df.Transaction < 0.]).Transaction.agg([sum])\n",
    "print('The total cash intake is %e pounds,\\n the total amount from cancellations is %e (%f )' \\\n",
    "        %(transaction_plus,transaction_cancel,-100*transaction_cancel/transaction_plus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude cancellations to get the spectrum of single items price\n",
    "# motivation: the cancellation would give rise to a double counting\n",
    "\n",
    "# NOTE: cancellations are sometimes negative Quantity for positive price,\n",
    "# other times they are positive Quantity and negative price\n",
    "tit='Unit Price'\n",
    "#(df.loc[ (df.Price > 0.) & (df.Quantity > 0)]).Price.plot(kind='hist',logy=True,title=tit,bins=50,figsize=(7,7))\n",
    "(df.loc[ (df.IsCancellation ==0 ) ]).Price.plot(kind='hist',logy=True,title=tit,bins=50,figsize=(7,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.loc[ (df.IsCancellation ==0 ) & (df['IsOrdinaryItem']==1)]).Price.plot(kind='hist',logy=True,title=tit,bins=50,figsize=(7,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraction_non_ordinary=100.*len(df.loc[  (df['IsOrdinaryItem']!=1)]) / len( (df.loc[  (df['IsOrdinaryItem']==1)])   )\n",
    "print('%f percent of the total items are non ordinary, defined as above %d or zero'%(fraction_non_ordinary,max_item_price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.loc[ (df.IsCancellation ==0 ) ]).Price.plot(kind='hist',logy=True,title=tit,bins=50,range=r,figsize=(7,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.loc[ (df.IsCancellation ==0 ) & (df['IsOrdinaryItem']==1) ]).Price.plot(kind='hist',logy=True,title=tit,bins=50,range=r,figsize=(7,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative logic: account for cancelled items, as if they were normal purchases\n",
    "tit='Unit Price (cancellations included)'\n",
    "df['Price'].map(lambda p: abs(p)) \\\n",
    "           .plot(kind='hist',logy=True,title=tit,bins=50,range=r,figsize=(7,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the transactions-or-cancellations above max_item_price\n",
    "# see them all\n",
    "# df.loc[ (df.Price > max_item_price) | (df.Price < -1*max_item_price) ].head(20000)\n",
    "\n",
    "# group them by Description to see what they are and how many \n",
    "df.loc[ (df.IsOrdinaryItem==0) & (df.Price>0) ].groupby('Description').Description.agg([len]) \\\n",
    "              .rename(columns={\"len\": \"item_huge_price\"})            \\\n",
    "              .sort_values(by=['item_huge_price'],ascending=False)\n",
    "\n",
    "\n",
    "# All prices/cancellatons in excess of 1500 are 'special accounting actions',\n",
    "# and are not actual goods being purchased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many ORDINARY items are on the cathalogue, based on unique description and price not set to 0\n",
    "len(df.loc[ df.IsOrdinaryItem==1 ].Description.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transactions with 0 prices --> all have odd/dodgy descriptions: manipulation errors and the like\n",
    "df.loc[ (df.IsOrdinaryItem==1) & (df.Price==0)]\n",
    "len(df.loc[ (df.IsOrdinaryItem==1) & (df.Price==0)])\n",
    "# this is 0 by construction, since IsOrdinaryItem flips to 0 of Price==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Several ordinary items are sold at different prieces; what fraction have different prices and what fraction has identical price?')\n",
    "\n",
    "def is_price_unique(row):\n",
    "    if row.min>=row.max and row.min<=row.max:\n",
    "        row.price_unique=100 \n",
    "        print('')\n",
    "\n",
    "        print(row.min)\n",
    "        print(row.max)\n",
    "        print(row)\n",
    "        print('--> the same')\n",
    "    else:\n",
    "        row.price_unique=999\n",
    "        print('')\n",
    "        print('')\n",
    "        print(row.min)\n",
    "        print(row.max)\n",
    "        print(row)\n",
    "        print('--> NOT the same')\n",
    "    return row\n",
    "# https://stackoverflow.com/questions/27474921/compare-two-columns-using-pandas\n",
    "\n",
    "#a = df.loc[ df.IsOrdinaryItem==1 ].groupby('Description').Price.agg([len,min,max,sum]) \\\n",
    "#                                  .sort_values(by=['sum'],ascending=False)             \\\n",
    "\n",
    "#b=a.apply(is_price_unique, axis='columns')\n",
    "#b\n",
    "\n",
    "# IN THE MAKING\n",
    "# TO DO: make a spectrum of the price for each element in the cathalogue\n",
    "\n",
    "# https://stackoverflow.com/questions/17578115/pass-percentiles-to-pandas-agg-function\n",
    "import numpy as np\n",
    "def percentile(n):\n",
    "    def percentile_(x):\n",
    "        return np.percentile(x, n)\n",
    "    percentile_.__name__ = 'percentile_%s' % n\n",
    "    return percentile_\n",
    "    \n",
    "df_items = df.loc[ df.IsOrdinaryItem==1 ].groupby('Description')        \\\n",
    "                         .Price.agg([len,min,           \\\n",
    "                                    percentile(10),    \\\n",
    "                                    percentile(25),    \\\n",
    "                                    percentile(50),    \\\n",
    "                                    percentile(75),    \\\n",
    "                                    percentile(90),    \\\n",
    "                                    max,sum])          \\\n",
    "                                    .rename(columns={\"sum\": \"item_revenue\",\"len\":\"item_num\"})  \n",
    "\n",
    "# this is an indication of the spread of the price of a given item\n",
    "df_items['percentile_7525'] = df_items['percentile_75'] / df_items['percentile_25'] \n",
    "df_items['percentile_9010'] = df_items['percentile_90'] / df_items['percentile_10'] \n",
    "\n",
    "df_items.loc[ df_items['item_revenue']>10000  ].sort_values(by=['item_revenue'],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how much money has been made with each given item\n",
    "# excluding the top three values which are not purchasable items\n",
    "tit='total revenue for a given item'\n",
    "r  = (0,20000)\n",
    "df_items['item_revenue'].plot(kind='hist',logy=True,title=tit,bins=50,range=r,figsize=(7,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many times has each item been sold ?\n",
    "tit='total num of sales per item'\n",
    "r  = (0,6000)\n",
    "df_items['item_num'].plot(kind='hist',logy=True,title=tit,bins=50,range=r,figsize=(7,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation between number of times an item has been sold and its single price ?\n",
    "df_items.plot.scatter(x='percentile_50',\n",
    "                      y='item_num',\n",
    "                      #c='species',\n",
    "                      #colormap='viridis'\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items['percentile_50'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this NOT a useful plot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist2d(df_items['percentile_50'].values,df_items['item_num'].values,\n",
    "              bins=(10,10),\n",
    "             #cmap=plt.cm.coolwarm\n",
    "              cmap=plt.cm.jet\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relative difference between the 75 and the 25 percentile prices\n",
    "# as function if item type, ordered by descending per-item total revenue\n",
    "# the first three items are not actual purchable items\n",
    "# \n",
    "df_items.loc[ (df_items['item_revenue']>7000) & (df_items['percentile_7525']<10) ]                        \\\n",
    "        .sort_values(by=['item_revenue'],ascending=False)[['percentile_7525','percentile_9010']]          \\\n",
    "        .plot.bar(rot=90,figsize=(15,10),logy=False,fontsize=20,ylim=(0,5),subplots=False)\n",
    "\n",
    "plt.rc('legend',fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_transactions\"></a>\n",
    "## Transaction carachteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tit='Transaction'\n",
    "df['Transaction'].plot(kind='hist',logy=True,title=tit,bins=100,figsize=(15,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import *\n",
    "\n",
    "tit='Revenue per Transaction'\n",
    "fig, ax = subplots()\n",
    "\n",
    "df.loc[ (df.Transaction >0) ].Transaction              \\\n",
    "       .plot(kind='hist',logy=True,title=tit,bins=100,  \\\n",
    "             range=(0,10000),figsize=(15,7),colormap='Accent',alpha=0.5,legend=True)\n",
    "\n",
    "df.loc[ (df.Transaction <0) ].Transaction.map(lambda u: -1*u)              \\\n",
    "       .plot(kind='hist',logy=True,title=tit,bins=100,  \\\n",
    "             range=(0,10000),figsize=(15,7),colormap='autumn',alpha=0.5,legend=True)\n",
    "\n",
    "ax.legend(['Transactions (positive earnings)', 'Cancellations'])\n",
    "print('Distribution of the amounts per recorded transaction. Take away message: at costs, the cancellations have larger relative importance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_invoicess\"></a>\n",
    "## Invoices carachteristics\n",
    "-- can be browsed to directly from here https://nbviewer.jupyter.org/github/franzoni/proton-Hands-on_task_Senior_Data_Scientist/blob/master/proton-EDA.ipynb#section_invoicess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tit='Invoice'\n",
    "df.groupby('Invoice').Transaction.agg([sum])    \\\n",
    "   .rename(columns={\"sum\": \"rev_invoice\"})            \\\n",
    "   .sort_values(by=['rev_invoice'],ascending=False)  \\\n",
    "   .plot(kind='hist',logy=True,title=tit,bins=100,range=(0,200000),figsize=(15,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tit='Invoice'\n",
    "\n",
    "fig, ax = subplots()\n",
    "\n",
    "df_invoices= df.groupby('Invoice').Transaction.agg([sum])    \\\n",
    "   .rename(columns={\"sum\": \"rev_invoice\"})            \\\n",
    "\n",
    "df_invoices.loc[ (df_invoices.rev_invoice >0) ]           \\\n",
    "        .sort_values(by=['rev_invoice'],ascending=False).rev_invoice  \\\n",
    "        .plot(kind='hist',logy=True,title=tit,bins=100,range=(0,30000),figsize=(15,7),colormap='Accent',alpha=0.5,legend=True)\n",
    "\n",
    "df_invoices.loc[ (df_invoices.rev_invoice <0) ]         \\\n",
    "        .sort_values(by=['rev_invoice'],ascending=False).rev_invoice.map(lambda u: -1*u)   \\\n",
    "        .plot(kind='hist',logy=True,title=tit,bins=100,range=(0,30000),figsize=(15,7),colormap='autumn',alpha=0.5,legend=True)\n",
    "\n",
    "ax.legend(['Invoices (positive earnings)', 'Invoice Cancellations'])\n",
    "print('Distribution of the amounts per recorded transaction. Take away message: at high costs, the cancellations have larger relative importance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_countries\"></a>\n",
    "## Countries carachteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are %d unique customers'%len(df.Customer_ID.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Customers are from %d countries'%len(df.Country.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toggle between the whole dataset (df) and the transactions w/ Customer_ID=Nan \n",
    "# to check for biases\n",
    "UU = df\n",
    "\n",
    "Cust_vs_country = UU.groupby('Country').Customer_ID.unique().agg([len])     \\\n",
    "                    .rename(columns={\"len\": \"num_customers\"})               \\\n",
    "                    .sort_values(by=['num_customers'],ascending=False)\n",
    "\n",
    "# I consider here only transactions with positive proceeds\n",
    "# I'll make a dedicated plot/column here below for negative/cancelled goods\n",
    "Revenue_vs_country = (UU.loc[UU.Transaction > 0.])                        \\\n",
    "                     .groupby('Country').Transaction.agg([sum])           \\\n",
    "                    .rename(columns={\"sum\": \"revenue\"})                     \\\n",
    "                    .sort_values(by=['revenue'],ascending=False)\n",
    "\n",
    "Cancellation_vs_country = (UU.loc[UU.Transaction < 0.])                \\\n",
    "                          .groupby('Country').Transaction.agg([sum])    \\\n",
    "                          .rename(columns={\"sum\": \"cancellation\"})            \\\n",
    "                          .sort_values(by=['cancellation'],ascending=False)\n",
    "\n",
    "# I chose to count ANY item listed, be it purchased or cancelled\n",
    "Items_vs_country = UU.groupby('Country').Invoice.agg([len])    \\\n",
    "                    .rename(columns={\"len\": \"num_items\"})            \\\n",
    "                    .sort_values(by=['num_items'],ascending=False)\n",
    "\n",
    "# I chose to count ANY item listed, be it purchased or cancelled\n",
    "Invoices_vs_country = UU.groupby('Country').Invoice.unique().agg([len])    \\\n",
    "                    .rename(columns={\"len\": \"num_invoices\"})            \\\n",
    "                    .sort_values(by=['num_invoices'],ascending=False)\n",
    "\n",
    "NoCustomerID_vs_country = df[df.isnull().any(axis=1)]                \\\n",
    "                    .groupby('Country').Transaction.agg([sum])     \\\n",
    "                    .rename(columns={\"sum\": \"no_customer_id\"})            \\\n",
    "                    .sort_values(by=['no_customer_id'],ascending=False) \\\n",
    "\n",
    "df_vs_country = Cust_vs_country              \\\n",
    "                .join(Revenue_vs_country )    \\\n",
    "                .join(Cancellation_vs_country)    \\\n",
    "                .join(Items_vs_country)   \\\n",
    "                .join(Invoices_vs_country)   \\\n",
    "                .join(NoCustomerID_vs_country,how='outer')   \\\n",
    "                .fillna(0)                                  \\\n",
    "                .sort_values(by=['revenue'],ascending=False) \\\n",
    "\n",
    "df_vs_country['rel_cancellation']   = \\\n",
    "           -100*df_vs_country['cancellation']/df_vs_country['revenue']\n",
    "\n",
    "df_vs_country['rel_no_customer_id'] = \\\n",
    "            100*df_vs_country['no_customer_id']/df_vs_country['revenue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NoCustomerID_vs_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vs_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.w3resource.com/pandas/dataframe/dataframe-plot-bar.php\n",
    "# good reference for plotting directly from pandas (less code!)\n",
    "df_vs_country.plot.pie(y='num_customers', figsize=(3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax= df_vs_country.drop(['cancellation','no_customer_id'], axis=1)  \\\n",
    "    .plot.bar(rot=90,figsize=(15,25),logy=True,fontsize=20,subplots=True)\n",
    "\n",
    "ax[0].set(ylabel = 'num. customers')\n",
    "ax[1].set(ylabel = 'revenue')\n",
    "ax[2].set(ylabel = 'num. items sold')\n",
    "ax[3].set(ylabel = 'num. invoices')\n",
    "ax[4].set(ylabel = 'canc. revenue [%]')\n",
    "# ax[4].set_yscale('linear')\n",
    "ax[5].set(ylabel = 'no_cus_id. revenue [%]')\n",
    "#ax[5].set_yscale('linear')\n",
    "\n",
    "for v in range(5):\n",
    "    ax[v].yaxis.get_label().set_fontsize(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_time\"></a>\n",
    "## Date/Time carachteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('InvoiceDay').Transaction.agg(sum).plot(figsize=(25, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('InvoiceWeekDay').Transaction.agg(sum).plot(figsize=(25, 12))\n",
    "print('Thursday is the day with the largest revenue, Saturday is the least, with near zero!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('InvoiceTime').Transaction.agg(sum)    \\\n",
    "               .plot(figsize=(25, 3))  # must be limited to >0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('InvoiceWeek').Transaction.agg(sum).plot(figsize=(25, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the largest customer by total net revenue (i.e. taking into account also cancellations, in negative)\n",
    "# is in the UK and has ID 18102.0 and has purchased for 600k pounds\n",
    "df.groupby('Customer_ID').Transaction.agg([sum,min,max,len])                                                  \\\n",
    "        .rename(columns={'sum': 'Total Customer Revenue',                                                     \\\n",
    "                         'min': 'Min Transaction','max': 'Max Transaction',                                   \\\n",
    "                            'len' : 'Number of Transaction'})                                                 \\\n",
    "              .sort_values(by=['Total Customer Revenue'],ascending=False).head(10)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the customer with the largest number of transactions is \n",
    "# is in the UK, has ID 17841.0 and has carried out 13.1k transactions\n",
    "df.groupby('Customer_ID').Transaction.agg([sum,min,max,len])                                                  \\\n",
    "        .rename(columns={'sum': 'Total Customer Revenue',                                                     \\\n",
    "                         'min': 'Min Transaction','max': 'Max Transaction',                                   \\\n",
    "                            'len' : 'Number of Transaction'})                                                 \\\n",
    "              .sort_values(by=['Number of Transaction'],ascending=False).head(10)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the customer with the largest number of transactions is \n",
    "# is in the UK, has ID 17841.0 and has carried out 12.9 transactions (excluding cancellation transactions)\n",
    "df.loc[df.Transaction>0]                                                                                      \\\n",
    "    .groupby('Customer_ID').Transaction.agg([sum,min,max,len])                                                \\\n",
    "        .rename(columns={'sum': 'Total Customer Revenue',                                                     \\\n",
    "                         'min': 'Min Transaction','max': 'Max Transaction',                                   \\\n",
    "                            'len' : 'Number of Transaction'})                                                 \\\n",
    "              .sort_values(by=['Number of Transaction'],ascending=False).head(10)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the UK generates the largest revenue (16M over the two years),\n",
    "# surprisingly EIRE is the second largest source of revenue, with 0.6M (despite having only 6 customers, they must be huge customers)\n",
    "# df.groupby('Country').Transaction.sum().sort_values(ascending=False)\n",
    "# superseeded by a plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_customers\"></a>\n",
    "## Customers carachteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plan for customers\n",
    "- make a dataframe\n",
    "- add also the RFM variables\n",
    "- add nuber of items bought in total, number of different TYPEs of items\n",
    "- add total value of course\n",
    "- Can I cathegorise the purchasable items? => If so, customers split across those cathegories\n",
    "- Persona; how many cheap items, few expensive ones?\n",
    "- https://cxl.com/blog/creating-customer-personas-using-data-driven-research/\n",
    "- now many items are bought per session, how much is spent per session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Customer_ID').size().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='the_destination'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Pippo goes to the cinema](section-title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
