{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to get these notebooks to work\n",
    "\n",
    "Dataset describe here: http://archive.ics.uci.edu/ml/datasets/Online+Retail+II\n",
    "\n",
    "wget http://archive.ics.uci.edu/ml/machine-learning-databases/00502/online_retail_II.xlsx\n",
    "\n",
    "git clone git@github.com:franzoni/proton-Hands-on_task_Senior_Data_Scientist.git\n",
    "\n",
    "mv online_retail_II.xlsx proton-Hands-on_task_Senior_Data_Scientist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_path = os.getcwd()\n",
    "print('++ Directory of this notebook:')\n",
    "print('\\t%s'%data_path)\n",
    "\n",
    "excel_with_path  = data_path+'/online_retail_II.xlsx'\n",
    "pickle_with_path = data_path+'/online_retail_II.pkl'\n",
    "csv_with_path    = data_path+'/online_retail_II.csv'\n",
    "\n",
    "print('\\n++Full path of the data file:')\n",
    "print('\\tdata_path =%s\\n\\n'%excel_with_path)\n",
    "\n",
    "df = None\n",
    "\n",
    "if os.path.isfile(pickle_with_path):\n",
    "    print('-> picke already exists, much faster using it than opening excel files\\n')\n",
    "    df = pd.read_pickle(pickle_with_path)\n",
    "else:\n",
    "    print('-> picke does not exist, go to excel, and create it\\n')\n",
    "    df1 = pd.read_excel (excel_with_path,'Year 2009-2010')\n",
    "    df2 = pd.read_excel (excel_with_path,'Year 2010-2011')\n",
    "    df = pd.concat([df1, df2])\n",
    "    df.to_pickle(pickle_with_path)\n",
    "    print('-> picke done\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define transaction: total cash for a single 'row' of the dataset, i.e. item_price times the number of items bought\n",
    "df['Transaction'] = df.Quantity * df.Price\n",
    "df=df.rename(columns={\"Customer ID\": \"Customer_ID\"})\n",
    "\n",
    "# day, week and time are added for convenience of analysis later on\n",
    "from datetime import datetime\n",
    "df['InvoiceDay'] = df['InvoiceDate'].map(lambda p: p.date())\n",
    "df['InvoiceWeekDay'] = df['InvoiceDate'].map(lambda p: p.weekday())\n",
    "df['InvoiceTime'] = df['InvoiceDate'].map(lambda p: p.time())\n",
    "df['InvoiceWeek'] = df['InvoiceDate'].map(lambda p: p.isocalendar()[1]+52*(p.year-2010))\n",
    "\n",
    "# avoide negative weeks and start counting from the first week of the dataset, which starts from 01/12/2009\n",
    "df['InvoiceWeek'] = df['InvoiceWeek']+3\n",
    "\n",
    "\n",
    "# implement the definition of cancellation in the documentation:\n",
    "# http://archive.ics.uci.edu/ml/datasets/Online+Retail+II\n",
    "#      ==> \"If this code starts with the letter 'c', it indicates a cancellation.\"\"\n",
    "def is_cancellation(s):\n",
    "    \"\"\"\" implement the definition of cancellation in the documentation:\n",
    "          http://archive.ics.uci.edu/ml/datasets/Online+Retail+II\n",
    "          ==> \"If this code starts with the letter 'c', it indicates a cancellation.\"\"\"\n",
    "    if isinstance(s, int):\n",
    "        #print \"ordinary invoice\"\n",
    "        return 0\n",
    "    elif isinstance(s, unicode):\n",
    "        #print \"unicode string\"\n",
    "        #thse are cancellations\n",
    "        if s.rfind('C')!=-1:\n",
    "            return 1\n",
    "        else:\n",
    "            # print \"Something unexpected\"  # found, e.g. A506401\n",
    "            # print s\n",
    "            return 2\n",
    "df['IsCancellation'] = df['Invoice'].map(is_cancellation)\n",
    "\n",
    "\n",
    "\n",
    "max_item_price=1400\n",
    "r             =(0,max_item_price)\n",
    "def is_ordinary_item(p):\n",
    "    \"\"\" items with prices above ~1500 are so few that is worth looking at them in detail, and excluding them from the plots\n",
    "        There'a lot of transactions with price set to 0, which based on the descriptions are \"\"\"\n",
    "    if abs(p)>max_item_price or p==0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "df['IsOrdinaryItem'] = df['Price'].map(is_ordinary_item)        \n",
    "\n",
    "# df.to_csv(csv_with_path, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_EDA_cleaning\"></a>\n",
    "# data cleaning: nan's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are the nulls in each feature?\n",
    "nulls = df.isnull().sum()[df.isnull().sum() != 0]\n",
    "\n",
    "# select all transactions containing \n",
    "df_nulls = df[df.isnull().any(axis=1)].copy(deep=True)\n",
    "\n",
    "nulls_rel = nulls/df.shape[0]*100\n",
    "\n",
    "nulls_summary = pd.concat([nulls, nulls_rel], axis=1, keys=['nulls', 'rel. nulls [%]'])\\\n",
    "               .sort_values('nulls', ascending=False)\n",
    "\n",
    "nulls_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## todo\n",
    "- nan and empty -> cleanup   # IN THE MAKING -> only consequences on the users study\n",
    "- add colums for data and time, separately ?    # DONE\n",
    "- create a second DF by user: RFM added to\n",
    "\n",
    "\n",
    "# What do I want to extract from this dataset? WORKING NOTES\n",
    "- https://en.wikipedia.org/wiki/Exploratory_data_analysis\n",
    "\n",
    "- how many customers   DONE, also as a function of country\n",
    "- make a pliot of transaction, of purchase prices, and of invoice_price! #  DONE\n",
    "\n",
    "- VS TIME: trends of spending overall, by country\n",
    "- revenue as a function of date, as a function of the time in the day  # DONE\n",
    "- trends of spending: overall, by the largest customer, by the smaller customers (TOO SPECIFIC?)\n",
    "\n",
    "- NEED TO BUILD A PER CUSTOMER DF\n",
    "- customer: how many transactions, how much total revenue:\n",
    "    => BREAK DOWN BY country, tra\n",
    "- RMF\n",
    "\n",
    "\n",
    "- ==> do this\n",
    "- how many different types of items                  # DONE\n",
    "- what kind of items are bought the most (by NUMBER of by REVENUE), are cancelled the most, \n",
    "- how much revenue per type of item bought\n",
    "\n",
    "- correlation between\n",
    "- cancellations: fraction of cancellation by nunber of transactions and by proportion of renenue\n",
    "-                correlation to CHURNING ? Correlation to country OR type of good purchased ?\n",
    "\n",
    "- Can I cathegorise the purchasable items? => If so, customers split across those cathegories\n",
    "- Persona; how many cheap items, few expensive ones?\n",
    "- https://cxl.com/blog/creating-customer-personas-using-data-driven-research/\n",
    "- now many items are bought per session, how much is spent per session\n",
    "\n",
    "==> TOWARDS CUSTOMER PERSONA DEFINITION\n",
    "- MANY TRANSACTIONS ARE RECORDED AT THE SAME TIME => as if they were a single shopping session, but billed in split goups of goods\n",
    "- the function summary_data_from_transaction_data treats transactions taking place on the same day as A SINGLE ONE!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_customers\"></a>\n",
    "## Customers carachteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup of data for customer analysis\n",
    "- we decide to remove all datata points with no Customer_ID\n",
    "- and to limit the study only to IsOrdinaryItem (as defined above), to avoid skewing the resuts w/ dataa points which are not actual purchases\n",
    "- to keep the modelling basic, for now we decide also to remove all cancellation oders\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df.loc[ (df.IsCancellation==0) & (df.IsOrdinaryItem==1) ]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.sort_values(by=['InvoiceDate'],ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic customer integral features\n",
    "- number of items bought trgought the entire period covered by data, indicating the overall commercial activity of each individual\n",
    "- unique items, indicates the variety of the customers interests\n",
    "- total amount spent by a customer\n",
    "- number of invoices, i.e. how many times a customers surfaced and made an order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are %d customers in the cleaned dataset'%df['Customer_ID'].nunique() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer value, diversity of interests, number of transactions and invoices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_customer_items  = df.groupby('Customer_ID').StockCode.agg([len])     \\\n",
    "                         .rename(columns={\"len\": \"items\"})               \\\n",
    "                         .sort_values(by=['items'],ascending=False)\n",
    "\n",
    "tit='Num items bought by customer'\n",
    "r=(0,3000)\n",
    "df_customer_items.plot(kind='hist',logy=True,title=tit,bins=50,range=r,colormap='Accent',alpha=0.5,figsize=(7,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_customer_uniq_items  = df.groupby('Customer_ID').StockCode.unique().agg([len])     \\\n",
    "                         .rename(columns={\"len\": \"uniq_items\"})               \\\n",
    "                         .sort_values(by=['uniq_items'],ascending=False)\n",
    "\n",
    "tit='Num unique items bought by customer'\n",
    "\n",
    "r=(0,750)\n",
    "df_customer_uniq_items.plot(kind='hist',logy=True,title=tit, \\\n",
    "                            bins=50,range=r,colormap='Accent',alpha=0.5,figsize=(7,7))\n",
    "plt.xlabel('Number of items') \n",
    "df_customer_uniq_items.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_customer_invoices  = df.groupby('Customer_ID').Invoice.unique().agg([len])     \\\n",
    "                         .rename(columns={\"len\": \"invoices\"})               \\\n",
    "                         .sort_values(by=['invoices'],ascending=False)\n",
    "\n",
    "tit='Num invoices per customer'\n",
    "r=(0,160)\n",
    "df_customer_invoices.plot(kind='hist',logy=True, \\\n",
    "                          title=tit,bins=50,range=r,colormap='Accent',alpha=0.5,figsize=(7,7))\n",
    "plt.xlabel('Number of invoices') \n",
    "df_customer_invoices.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer_items['invoices_cl'] = df_customer_invoices['invoices']\n",
    "df_customer_items['itemsPerInvoice'] = df_customer_items['items']/df_customer_items['invoices_cl']\n",
    "\n",
    "df_customer_items.head()\n",
    "\n",
    "tit='Num items per invoice'\n",
    "r=(0,160)\n",
    "df_customer_items.itemsPerInvoice.plot(kind='hist',logy=True, \\\n",
    "                          title=tit,bins=50,range=r,colormap='Accent',alpha=0.5,figsize=(7,7))\n",
    "plt.xlabel('Num items per invoice') \n",
    "df_customer_items.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer_spent = df.groupby('Customer_ID').Transaction.agg([sum])     \\\n",
    "                   .rename(columns={\"sum\": \"total_spent\"})               \\\n",
    "                   .sort_values(by=['total_spent'],ascending=False)\n",
    "\n",
    "tit='total amount spent by a customer'\n",
    "r=(0,100000)\n",
    "df_customer_spent.plot(kind='hist',logy=True,title=tit, \\\n",
    "                       bins=50,range=r,colormap='Accent',alpha=0.5,figsize=(7,7))\n",
    "plt.xlabel('amount spent [Pounds]') \n",
    "df_customer_spent.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer_spent.sort_values(by=['Customer_ID'],ascending=True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[ (df.Customer_ID==12346) &  (df.Quantity<100000)].Transaction.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation by spot checks: on a few specific customers to ensure there's no bugs\n",
    "df.loc[ (df.Customer_ID==12346)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[ (df.Customer_ID==12346)].InvoiceDay.unique().shape\n",
    "# verification of 'fequency' being equal to 7 for 12346\n",
    "# remiminder: the RFM variables exclude the first invoice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Historic customer  features\n",
    "\n",
    "## The literature is rich in models and prescriptions\n",
    "- see for instance: https://www.blastanalytics.com/blog/rfm-analysis-boosts-sales\n",
    "- and https://joaocorreia.io/blog/rfm-analysis-increase-sales-by-segmenting-your-customers.html\n",
    "\n",
    "## to characterise customers according to three simple historical variables \n",
    "- **Recency**: How recently a customer has made a purchase\n",
    "- **Frequency**: How often a customer makes a purchase\n",
    "- **Monetary Value**: Average expenditure for an invoice\n",
    "\n",
    "# Notes\n",
    "- RFM variables exlcude the first invoice for all customers. Only returning invoices are considered\n",
    "- note2: values=0 are for customers that don't return => select those customers away to consider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation of data: two attempts to cluster customers\n",
    "- both relying on RFM data, which I am about to set up now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --user lifetimes\n",
    "print('pip install worked out fine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_star='2009-12-01'\n",
    "calib_end='2011-05-31'\n",
    "obser_end='2011-12-09'\n",
    "\n",
    "from datetime import date\n",
    "d_data_star=date( * map(lambda u:int(u), data_star.split('-') ) )\n",
    "d_calib_end=date( * map(lambda u:int(u), calib_end.split('-') ) )\n",
    "d_obser_end=date( * map(lambda u:int(u), obser_end.split('-') ) )\n",
    "\n",
    "from lifetimes.utils import *\n",
    "\n",
    "#    This transforms a DataFrame of transaction data of the form:\n",
    "#        customer_id, datetime [, monetary_value]\n",
    "#    to a DataFrame of the form:\n",
    "#        customer_id, frequency, recency, T [, monetary_value]\n",
    "\n",
    "df_customer_RFM = summary_data_from_transaction_data(df.sort_values(by=['InvoiceDate'],ascending=True)\n",
    "                                                     ,'Customer_ID','InvoiceDate'\n",
    "                                                     ,monetary_value_col='Transaction'\n",
    "                                                     ,freq='D'\n",
    "                                                     #,observation_period_end=obser_end\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer_RFM.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tit='(Monetary Value) Average expenditure for an invoice'\n",
    "r=(0,3000)\n",
    "df_customer_RFM['monetary_value'].loc[df_customer_RFM['monetary_value']>0]  \\\n",
    "                                 .plot(kind='hist',logy=True,title=tit,     \\\n",
    "                                       bins=50,range=r,colormap='autumn',alpha=0.5,figsize=(7,7))\n",
    "\n",
    "plt.xlabel('Monetary Value [Pounds]') \n",
    "df_customer_RFM['monetary_value'].loc[df_customer_RFM['monetary_value']>0].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tit='Recency (How recently a customer has made last purchase)'\n",
    "r=(0,800)\n",
    "df_customer_RFM['recency'].loc[df_customer_RFM['recency']>0] \\\n",
    "     .plot(kind='hist',logy=True,title=tit,bins=50,          \\\n",
    "           range=r,colormap='autumn',alpha=0.5,figsize=(14,7))\n",
    "plt.xlabel('Recency [Days]') \n",
    "df_customer_RFM['recency'].loc[df_customer_RFM['recency']>0].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tit='Frequency (how many times a customer has come back)'\n",
    "r=(0,100)\n",
    "df_customer_RFM['frequency'].loc[df_customer_RFM['frequency']>0] \\\n",
    "           .plot(kind='hist',logy=True,title=tit,bins=50,range=r,colormap='autumn',alpha=0.5,figsize=(7,7))\n",
    "plt.xlabel('Frequency') \n",
    "df_customer_RFM['frequency'].loc[df_customer_RFM['frequency']>0].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here's where I aggregate all specific dataframes to come up with the oveall VIEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer = df_customer_items     \\\n",
    "                .join(df_customer_invoices)    \\\n",
    "                .join(df_customer_spent)   \\\n",
    "                .join(df_customer_RFM)   \\\n",
    "            .sort_values(by=['total_spent'],ascending=False)\n",
    "\n",
    "df_customer.shape\n",
    "df_customer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# consider only returning customers\n",
    "- Keep the model simple and don''t treat customers who have never come back => require frequency and recency to be positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer = df_customer.loc[df_customer['monetary_value']>0]   \\\n",
    "                    .loc[df_customer['recency']>0]          \\\n",
    "                    .loc[df_customer['frequency']>0]        \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are %d RETURNING customers in the cleaned dataset'%df_customer['monetary_value'].count() )\n",
    "df_customer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer.keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(df_customer, alpha=0.2,figsize=(15,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cut_out=0.02\n",
    "# q_low = df_customer[\"monetary_value\"].quantile(cut_out)\n",
    "# q_hi  = df_customer[\"monetary_value\"].quantile(1-cut_out)\n",
    "# df_customer = df_customer[(df_customer[\"monetary_value\"] < q_hi) & (df_customer[\"monetary_value\"] > q_low)]\n",
    "# df_customer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assess the correlation between existing variables\n",
    "# to makea choice of which variables to keep and discart in the clustering approach (see later on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(df_customer.drop([],axis=1).corr(),cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We keep only the 3 RFM and 'total spent'\n",
    "- adding other variables would not add significan rather pollute w/ noise from highly correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df_customer[['frequency','monetary_value','recency','total_spent']].corr(),cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduce logarithm of features\n",
    "- to better surface the relevance of features which are distributed exponentially\n",
    "- otherwise the constraining power of most features remains hidden are otherwise hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "do_log = True\n",
    "if do_log:\n",
    "    df_customer['monetary_value_log'] = np.log(df_customer['monetary_value'])\n",
    "    df_customer['frequency_log'] = np.log(df_customer['frequency'])\n",
    "    df_customer['total_spent_log'] = np.log(df_customer['total_spent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer.keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "# df_customer_km = df_customer.drop(['Customer_ID'],axis=0)\n",
    "\n",
    "mat_customer_km = df_customer[['frequency_log','recency','monetary_value_log','total_spent_log']]\n",
    "mat_customer_km.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(df_customer[['frequency_log','recency','monetary_value_log','total_spent_log']], alpha=0.2,figsize=(15,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPROACH 1: basic grouping of customers\n",
    "- cut-and-count method using RFM variables and their quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.blastanalytics.com/blog/rfm-analysis-boosts-sales\n",
    "# https://joaocorreia.io/blog/rfm-analysis-increase-sales-by-segmenting-your-customers.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to assign quantile bins, for the case of 4 and 3 bins\n",
    "\n",
    "# such that 1/4 score means best/worst customer feature\n",
    "def scoring_four(x,p,d,invert):\n",
    "    \"\"\" the last argument in the API flips the order of the score,\n",
    "        such that LARGE score always means GOOD customer feature\"\"\"\n",
    "    if x <= d[p][0.25]:\n",
    "        return 1 if invert else 4\n",
    "    elif x <= d[p][0.50]:\n",
    "        return 2 if invert else 3\n",
    "    elif x <= d[p][0.75]: \n",
    "        return 3 if invert else 2\n",
    "    else:\n",
    "        return 4 if invert else 1\n",
    "\n",
    "# such that 1/3 score means best/worst customer feature\n",
    "def scoring_three(x,p,d,invert):\n",
    "    \"\"\" the last argument in the API flips the order of the score,\n",
    "        such that LARGE score always means GOOD customer feature\"\"\"\n",
    "    if x <= d[p][0.33]:\n",
    "        return 1 if invert else 3\n",
    "    elif x <= d[p][0.66]:\n",
    "        return 2 if invert else 2\n",
    "    else:\n",
    "        return 3 if invert else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = df_customer.quantile(q=[0.25,0.5,0.75])\n",
    "quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = df_customer.quantile(q=[0.33,0.66])\n",
    "quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles_d = quantiles.to_dict()\n",
    "#quantiles_d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer['recency_score']   = df_customer['recency']             .apply(scoring_three, args=('recency',quantiles,True))\n",
    "df_customer['frequency_score'] = df_customer['frequency']           .apply(scoring_three, args=('frequency',quantiles,False))\n",
    "df_customer['monetary_value_score'] = df_customer['monetary_value'] .apply(scoring_three, args=('monetary_value',quantiles,False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer.groupby(['recency_score','frequency_score','monetary_value_score'], sort = True).T.agg(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer.groupby(['monetary_value_score','recency_score',], sort = True) \\\n",
    "    .T.agg(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_customer.groupby(['monetary_value_score','frequency_score',], sort = True).T.agg(len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPROACH 2: Customer clustering: k-mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the elbow method to find the optimal number of clusters\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import  silhouette_score\n",
    "\n",
    "wcss = []\n",
    "mat_customer_km = mat_customer_km.as_matrix()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)\n",
    "    kmeans.fit(mat_customer_km)\n",
    "    clusters = kmeans.predict(mat_customer_km)\n",
    "    if i>1:\n",
    "        silhouette_avg = silhouette_score(mat_customer_km, clusters)\n",
    "        print(\"For n_clusters =\", i, \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "    wcss.append(kmeans.inertia_)\n",
    "    \n",
    "plt.plot(range(1, 11), wcss)\n",
    "plt.title('Elbow plot')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=mat_customer_km\n",
    "# Fitting K-Means to the dataset\n",
    "how_many_clusters = 3\n",
    "\n",
    "kmeans = KMeans(n_clusters = how_many_clusters, init = 'k-means++', random_state = 42)\n",
    "y_kmeans = kmeans.fit_predict(X)\n",
    "\n",
    "\n",
    "# 0: frequency\n",
    "# 1: recency\n",
    "# 2: value\n",
    "# 3: total\n",
    "labels = ['log frequency','recency [days]','log value','log total']\n",
    "\n",
    "fig, axs = plt.subplots(1,3)\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(16)\n",
    "fig.suptitle('Clusters of customers (number of clusters = %d)'%how_many_clusters)\n",
    "\n",
    "for x_index,y_index,u in [[0,3,0],[1,3,1],[2,3,2]]:\n",
    "    \n",
    "    # Visualising the clusters\n",
    "    axs[u].scatter( X[y_kmeans == 1, x_index], X[y_kmeans == 1, y_index], s = 2, c = 'blue', label = 'Cluster 2')\n",
    "    axs[u].scatter( X[y_kmeans == 2, x_index], X[y_kmeans == 2, y_index], s = 2, c = 'green', label = 'Cluster 3')\n",
    "    axs[u].scatter( X[y_kmeans == 0, x_index], X[y_kmeans == 0, y_index], s = 2, c = 'red', label = 'Cluster 1')\n",
    "\n",
    "    #axs[u].scatter(X[y_kmeans == 3, x_index], X[y_kmeans == 3, y_index], s = 2, c = 'cyan', label = 'Cluster 4')\n",
    "    #axs[u].scatter(X[y_kmeans == 4, x_index], X[y_kmeans == 4, y_index], s = 2, c = 'magenta', label = 'Cluster 5')\n",
    "\n",
    "    # barycenters and labels\n",
    "    axs[u].scatter(kmeans.cluster_centers_[:, x_index], kmeans.cluster_centers_[:, y_index], s = 200, c = 'orange', label = 'Centroids')\n",
    "\n",
    "    axs[u].set_xlabel(labels[x_index])\n",
    "    axs[u].set_ylabel(labels[y_index])\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# interpretation and conclusions\n",
    "- Customers in cluster number 2 have shopped long ago ( at least 500 days before the end of the period covered by the data) and have the largest shopping frequency and total expenditure\n",
    "- Customers in cluster number 3 have shopped most recently (at most 250 days ago) and tend to have a low shopping frequency (both visible in the first plot, and the the third where total/averag expenditure is low)\n",
    "- Customers in cluster number 1 have an intermediate behaviour both in terms of recency (between 250 and 500) and frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_kmeans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freq\n",
    "X[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recency\n",
    "X[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# value\n",
    "X[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Proton assignment - April 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup of data for customer analysis\n",
    "- we decide to remove all datata points with no Customer_ID\n",
    "- and to limit the study only to IsOrdinaryItem (as defined above), to avoid skewing the resuts w/ dataa points which are not actual purchases\n",
    "- to keep the modelling basic, for now we decide also to remove all cancellation oders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape is ok\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[ (df.Customer_ID==12346.0)  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_customer_items  = df.groupby('Customer_ID').StockCode.agg([len])     \\\n",
    "#                          .rename(columns={\"len\": \"items\"})               \\\n",
    "#                         .sort_values(by=['items'],ascending=False)\n",
    "df_first_time=df.groupby(['Customer_ID']).InvoiceDate.agg(min)\n",
    "df_first_time.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.groupby(df['A']).apply(lambda x : x.iloc[1] if len(x) >= 2 else x.iloc[0])\n",
    "df_date_sorted = df.sort_values(by=['InvoiceDate'])\n",
    "df_second_time = df_date_sorted.groupby(['Customer_ID']).InvoiceDate.unique().apply(lambda x : x[1] if len(x) >= 2 else '2020-04-15 11:00:00')\n",
    "\n",
    "# df_customer_spent = df.groupby('Customer_ID').Transaction.agg([sum])     \\\n",
    "#                   .rename(columns={\"sum\": \"total_spent\"})               \\\n",
    "#                   .sort_values(by=['total_spent'],ascending=False)\n",
    "\n",
    "df_second_time.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_first_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_customer = df_customer_items     \\\n",
    "#                 .join(df_customer_invoices)    \\\n",
    "#                 .join(df_customer_spent)   \\\n",
    "#                 .join(df_customer_RFM)   \\\n",
    "#            .sort_values(by=['total_spent'],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wfi = df.merge(df_first_time,left_on='Customer_ID', right_on='Customer_ID',suffixes=('', '_first_pur'))\n",
    "# .merge(df2, left_on='lkey', right_on='rkey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wfi.head(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wse_bare = df_wfi.merge(df_second_time,left_on='Customer_ID', right_on='Customer_ID',suffixes=('', '_second_pur'))\n",
    "df_wse_bare.head(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wse = df_wse_bare.merge(df_customer_RFM,left_on='Customer_ID', right_on='Customer_ID',suffixes=('', '_rfm'))\n",
    "df_wse.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_only_first_purchases = df_wse.loc[df_wse['InvoiceDate'] == df_wse['InvoiceDate_first_pur']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_only_first_purchases.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_only_second_purchases = df_wse.loc[df_wse['InvoiceDate'] == df_wse['InvoiceDate_second_pur']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_only_second_purchases.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(df_only_first_purchases[['Quantity','InvoiceDate','Price','Country','Transaction']], alpha=0.2,figsize=(15,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxq    = 100\n",
    "maxtran = 250\n",
    "pd.plotting.scatter_matrix(df_only_first_purchases[['Quantity','InvoiceDate','Price','Country','Transaction','recency']] \\\n",
    "                           [(df_only_first_purchases.recency<250) & (df_only_first_purchases.Quantity<maxq)  & (df_only_first_purchases.Price<150) & (df_only_first_purchases.Transaction<maxtran) ], \\\n",
    "                           alpha=0.2,figsize=(15,15),color='green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(df_only_first_purchases[['Quantity','InvoiceDate','Price','Country','Transaction','recency']] \\\n",
    "                           [(df_only_first_purchases.recency>=250) & (df_only_first_purchases.recency<510) & (df_only_first_purchases.Quantity<maxq)  & (df_only_first_purchases.Price<150) & (df_only_first_purchases.Transaction<maxtran)   ], \\\n",
    "                           alpha=0.2,figsize=(15,15),color='green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(df_only_first_purchases[['Quantity','InvoiceDate','Price','Country','Transaction','recency']] \\\n",
    "                           [(df_only_first_purchases.recency>510)  & (df_only_first_purchases.Quantity<maxq)  & (df_only_first_purchases.Price<150) & (df_only_first_purchases.Transaction<maxtran) ], \\\n",
    "                           alpha=0.2,figsize=(15,15),color='green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(df_only_second_purchases[['Quantity','InvoiceDate','Price','Country','Transaction','recency']] \\\n",
    "                           [(df_only_second_purchases.recency<250)   & (df_only_second_purchases.Quantity<maxq)  & (df_only_second_purchases.Price<150) & (df_only_second_purchases.Transaction<maxtran) ], \\\n",
    "                           alpha=0.2,figsize=(15,15),color='orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(df_only_second_purchases[['Quantity','InvoiceDate','Price','Country','Transaction','recency']] \\\n",
    "                           [(df_only_second_purchases.recency<510) & (df_only_second_purchases.recency>=250) & (df_only_second_purchases.Quantity<maxq)  & (df_only_second_purchases.Price<150) & (df_only_second_purchases.Transaction<maxtran) ], \\\n",
    "                           alpha=0.2,figsize=(15,15),color='orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(df_only_second_purchases[['Quantity','InvoiceDate','Price','Country','Transaction','recency']] \\\n",
    "                           [(df_only_second_purchases.recency>=510)& (df_only_second_purchases.Quantity<maxq)  & (df_only_second_purchases.Price<150) & (df_only_second_purchases.Transaction<maxtran)   ], \\\n",
    "                           alpha=0.2,figsize=(15,15),color='orange')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
