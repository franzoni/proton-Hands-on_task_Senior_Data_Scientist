{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to get these notebooks to work\n",
    "\n",
    "Dataset describe here: http://archive.ics.uci.edu/ml/datasets/Online+Retail+II\n",
    "\n",
    "wget http://archive.ics.uci.edu/ml/machine-learning-databases/00502/online_retail_II.xlsx\n",
    "\n",
    "git clone git@github.com:franzoni/proton-Hands-on_task_Senior_Data_Scientist.git\n",
    "\n",
    "mv online_retail_II.xlsx proton-Hands-on_task_Senior_Data_Scientist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA cleaning and preparation\n",
    "## repeat the choices already made fr EDA and customers analyes, namely:\n",
    "- remove nulls\n",
    "- consider only ordinary transactions\n",
    "- remove cancellations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import pandas as pd\n",
    "\n",
    "data_path = os.getcwd()\n",
    "print('++ Directory of this notebook:')\n",
    "print('\\t%s'%data_path)\n",
    "\n",
    "excel_with_path  = data_path+'/online_retail_II.xlsx'\n",
    "pickle_with_path = data_path+'/online_retail_II.pkl'\n",
    "csv_with_path    = data_path+'/online_retail_II.csv'\n",
    "\n",
    "print('\\n++Full path of the data file:')\n",
    "print('\\tdata_path =%s\\n\\n'%excel_with_path)\n",
    "\n",
    "df = None\n",
    "\n",
    "if os.path.isfile(pickle_with_path):\n",
    "    print('-> picke already exists, much faster using it than opening excel files\\n')\n",
    "    df = pd.read_pickle(pickle_with_path)\n",
    "else:\n",
    "    print('-> picke does not exist, go to excel, and create it\\n')\n",
    "    df1 = pd.read_excel (excel_with_path,'Year 2009-2010')\n",
    "    df2 = pd.read_excel (excel_with_path,'Year 2010-2011')\n",
    "    df = pd.concat([df1, df2])\n",
    "    df.to_pickle(pickle_with_path)\n",
    "    print('-> picke done\\n')\n",
    "    \n",
    "df['Transaction'] = df.Quantity * df.Price\n",
    "df=df.rename(columns={\"Customer ID\": \"Customer_ID\"})\n",
    "\n",
    "# implement the definition of cancellation in the documentation:\n",
    "# http://archive.ics.uci.edu/ml/datasets/Online+Retail+II\n",
    "#      ==> \"If this code starts with the letter 'c', it indicates a cancellation.\"\"\n",
    "def is_cancellation(s):\n",
    "    if isinstance(s, int):\n",
    "        #print \"ordinary invoice\"\n",
    "        return 0\n",
    "    elif isinstance(s, unicode):\n",
    "        #print \"unicode string\"\n",
    "        #thse are cancellations\n",
    "        if s.rfind('C')!=-1:\n",
    "            return 1\n",
    "        else:\n",
    "            # print \"Something unexpected\"  # found, e.g. A506401\n",
    "            # print s\n",
    "            return 2\n",
    "df['IsCancellation'] = df['Invoice'].map(is_cancellation)     \n",
    "      \n",
    "\n",
    "\n",
    "# items with prices above ~1500 are so few that is worth looking at them in detail, and excluding them from the plots\n",
    "# There'a lot of transactions with price set to 0, which based on the descriptions are \n",
    "max_item_price=1400\n",
    "r             =(0,max_item_price)\n",
    "def is_ordinary_item(p):\n",
    "    if abs(p)>max_item_price or p==0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "df['IsOrdinaryItem'] = df['Price'].map(is_ordinary_item) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.dropna()\n",
    "df= df.loc[ (df.IsCancellation==0) & (df.IsOrdinaryItem==1) ]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Are there users at high risk of churning by the end of 2011\n",
    "\n",
    "- https://www.kdnuggets.com/2019/05/churn-prediction-machine-learning.html \n",
    "- https://towardsdatascience.com/hands-on-predict-customer-churn-5c2a42806266\n",
    "- https://www.google.com/search?q=dwarf+on+the+shoulders+of+giants&rlz=1C5CHFA_enCH771CH771&sxsrf=ALeKk034vLLmAHn5V1c0QAON4DZuO1GqVA:1582207255390&source=lnms&tbm=isch&sa=X&ved=2ahUKEwiRrayrpeDnAhXCxaYKHZQVCgcQ_AUoAXoECBIQAw&biw=1418&bih=915#imgrc=oj4GwW0aHYfVYM\n",
    "- similar exercise resolved here https://towardsdatascience.com/modeling-customer-churn-for-an-e-commerce-business-with-python-874315e688bf\n",
    "- the repo of the library to be used https://github.com/CamDavidsonPilon/lifetimes\n",
    "- the reference paper in pdf http://brucehardie.com/papers/018/fader_et_al_mksc_05.pdf http://brucehardie.com/papers/bgnbd_2004-04-20.pdf and the journal which has all aspects of a respectable journals \n",
    "- GF TODO: once you've found which customers have churned, make a few hitory plots to prove that indeed they have churned, to show that I am not blindly trusing the package I've downloade\n",
    "\n",
    "\n",
    "- cancellations: fraction of cancellation by nunber of transactions and by proportion of renenue\n",
    "-                correlation to CHURNING ? Correlation to country OR type of good purchased ?\n",
    "\n",
    "\n",
    "- NOT SURE IF USEFUL some stack-overflow like explanations about the Pareto model https://stats.stackexchange.com/questions/251506/is-it-possible-to-understand-pareto-nbd-model-conceptually\n",
    "\n",
    "## STRATEGY: \n",
    "- calibrate modelMID up to mid 2011\n",
    "- check the model on the last 6 months\n",
    "- modelMID gives the probability of alive/death at mid 2011\n",
    "- re-calibrate over the whole period, modelFULL\n",
    "- modelFULL gives the probability of alive/death at end\n",
    "- question 1: p(Alive, modelMID) is high, p(Alive, modelFULL) is low\n",
    "- question 2: p(Alive, modelFULL) is somewhat low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from lifetimes.utils import *\n",
    "from lifetimes import BetaGeoFitter,GammaGammaFitter\n",
    "from lifetimes.plotting import plot_probability_alive_matrix, plot_frequency_recency_matrix, plot_period_transactions, plot_cumulative_transactions,plot_incremental_transactions\n",
    "from lifetimes.generate_data import beta_geometric_nbd_model\n",
    "from lifetimes.plotting import plot_calibration_purchases_vs_holdout_purchases, plot_period_transactions,plot_history_alive\n",
    "from lifetimes.fitters import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(summary_data_from_transaction_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   RFM variables and negative/cancellation transactions\n",
    "- frequency, recency, T\n",
    "- how do I deal with transactions, in producing the RFM models ?\n",
    "- for now just exclude them: cancellations are likely to correlate to churning behaviour, however the frequency of positive transactions is what the pareto model uses and expects to predict churning probabilty\n",
    "- ===> check if there's correlation between churning behaviour and frequency of cancellations ?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FOR the moment I exclude cancellations from the type of transactions to compute RFM variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_forRFM = ['Customer_ID','InvoiceDate','Transaction']\n",
    "df_forRFM = df[columns_forRFM]\n",
    "df_forRFM.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forRFM.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DOC of function _summary__data__from__transaction__data_\n",
    "summary_data_from_transaction_data(transactions, customer_id_col, datetime_col, monetary_value_col=None, datetime_format=None, observation_period_end=None, freq='D', freq_multiplier=1)\n",
    "\n",
    "- Return summary data from transactions.\n",
    "    \n",
    "-    This transforms a DataFrame of transaction data of the form:\n",
    "        customer_id, datetime [, monetary_value]\n",
    "-    to a DataFrame of the form:\n",
    "        customer_id, frequency, recency, T [, monetary_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFM_total is for the total dataset\n",
    "RFM_total = summary_data_from_transaction_data(df_forRFM, \\\n",
    "                'Customer_ID','InvoiceDate'\n",
    "                ,monetary_value_col='Transaction',freq='D')\n",
    "RFM_total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFM_total.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RFM info\n",
    "\n",
    "GENERALLY, the first transaction (aka 'the birth' of the customer) is ecluded from the RFM variables\n",
    "\n",
    "DEFINITIONS\n",
    "- Recency: time between initial purchase and most recent (last) purchase\n",
    "- Frequency: number of repeat purchases made by a customer (total purchases — 1)\n",
    "- Monetary: total spent on purchases\n",
    "\n",
    "FUNCTION summary_data_from_transaction_data\n",
    "- it's a rather clever thing: it AGGREGATES into 1 all transactions taking place at the same DAY!\n",
    "\n",
    "- frequency: # of days in which a customer made a repeat purchase\n",
    "- T: customer's age in days\n",
    "- recency: customer's age in days at the time of the most recent purchase\n",
    "- monetary_value: mean of a customer's purchases, excluding the 1st purchase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a basic glance at the content\n",
    "RFM_total.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verifiaction of how summary_data_from_transaction_data\n",
    "- for a few Customer_Id randomly picked, Frequency makes sense\n",
    "- the code behaves ok based on the spot-checks, is publicly available https://github.com/CamDavidsonPilon/lifetimes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHOULD I RESTRICT MOST OF THE ANALYSIS AND PLOTS TO CUSTOMERS WHO HAVE RETURNED?\n",
    "- if a customer has haver come back, it's probably dead anyway...\n",
    "- and all the values at 0 are somewhat disturbing\n",
    "\n",
    "## DONE, can remove this box at some point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFM_total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep the option of toggling this\n",
    "if False:\n",
    "    RFM_total = RFM_total.loc[RFM_total['monetary_value']>0]   \\\n",
    "                        .loc[RFM_total['recency']>0]          \\\n",
    "                        .loc[RFM_total['frequency']>0]        \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFM_total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_forRFM[df_forRFM['Customer_ID']==12346.0].sort_values(by=['InvoiceDate'],ascending=False)\n",
    "#df_forRFM[df_forRFM['Customer_ID']==12346.0].InvoiceDate.unique()\n",
    "# len(df[df['Customer_ID']==12346.0].InvoiceDate.unique())\n",
    "df_forRFM[df_forRFM['Customer_ID']==12347.0].InvoiceDate.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFM_total.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tit='Num repeat purchases (tot - 1)'\n",
    "RFM_total['frequency'].plot(kind='hist',logy=True,bins=50,title=tit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tit='time bwn initial purchase and most recent (last) purchase'\n",
    "RFM_total['recency'].plot(kind='hist',logy=True,bins=50,title=tit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFM_total.plot.scatter(x='frequency',y='recency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(RFM_total, alpha=0.2,figsize=(15,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assumptions of the BG/NBD model:\n",
    "- A customer’s relationship has two phases: “alive” for an unobserved period of time, then “dead”\n",
    "- While alive, the number of transactions made by a customer follows a Poisson distribution with transaction rate lambda\n",
    "- Heterogeneity in lambda follows a gamma distribution\n",
    "- After any transaction, a customer dies with probability p; the probability that a customer dies after a number of transactions follows a geometric distribution\n",
    "- p follows a beta distribution\n",
    "- Lambda and p vary independently across customers\n",
    "\n",
    "# seen these references\n",
    "- https://medium.com/data-shopify/how-shopify-merchants-can-measure-retention-c12284bfed6f\n",
    "- http://brucehardie.com/papers/bgnbd_2004-04-20.pdf (of which I have other url's as well)\n",
    "\n",
    "\n",
    "# For follow up\n",
    "\n",
    "- how many customers are there with more than 50/75 purchases ?\n",
    "  Perhaps worth selecting those out and treating them separately ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(BetaGeoFitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_play = BetaGeoFitter(penalizer_coef=0.0)\n",
    "beta_play.fit(RFM_total['frequency'], RFM_total['recency'], RFM_total['T']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://github.com/CamDavidsonPilon/lifetimes/blob/master/lifetimes/plotting.py#L211\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plot_frequency_recency_matrix(beta_play);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "plot_probability_alive_matrix(beta_play);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare for _number of transactions_ :\n",
    "- prediction from the model, over period used for its fit\n",
    "- the actual data, over the period used for its fit\n",
    "## NOTE: this is a game done for the whole dataset, it's to learn how to play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf  = 100\n",
    "for ly in [True,False]:\n",
    "    # https://github.com/CamDavidsonPilon/lifetimes/blob/fb047f7471f07ca4416ddffdfeeca898404cda79/lifetimes/plotting.py#L25\n",
    "    plot_period_transactions(beta_play, max_frequency=mf\n",
    "                        ,title=\"Frequency of Repeat Transactions (total dataset)\"\n",
    "                        ,xlabel=\"Number of Calibration Period Transactions\"\n",
    "                        ,ylabel=\"Customers\"\n",
    "                        ,logy=ly\n",
    "                        ,figsize=(30,10)    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEPARATE data up-to June 2011 (calibration),\n",
    "# from the whole dataset up to december (total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_star='2009-12-01'\n",
    "calib_end='2011-06-30'\n",
    "obser_end='2011-12-09'\n",
    "\n",
    "from datetime import date\n",
    "d_data_star=date( * map(lambda u:int(u), data_star.split('-') ) )\n",
    "d_calib_end=date( * map(lambda u:int(u), calib_end.split('-') ) )\n",
    "d_obser_end=date( * map(lambda u:int(u), obser_end.split('-') ) )\n",
    "\n",
    "data_span = (d_obser_end - d_data_star).days\n",
    "data_train= (d_calib_end - d_data_star).days\n",
    "print('Data cover %d days overall, of which %d are used for training'%(data_span,data_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/CamDavidsonPilon/lifetimes/blob/master/lifetimes/utils.py#L27\n",
    "\n",
    "#  Create a summary of each customer over a calibration and test period.    \n",
    "summary_cal_test = calibration_and_holdout_data(df_forRFM,'Customer_ID','InvoiceDate',\n",
    "                                                    calibration_period_end=calib_end,\n",
    "                                                    observation_period_end=obser_end)\n",
    "\n",
    "summary_cal_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XCHECK THIS SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary_cal_test = summary_cal_test.loc[summary_cal_test['frequency_cal']>0]\n",
    "#summary_cal_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_cal_test.sort_values(by=['frequency_cal'],ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit the Beta-Geometric/NBD model, restricted up to June 2011\n",
    "- restrict data up to the first semester of 2011\n",
    "- fit the Beta-Geometric/NBD over that period\n",
    "- use the remaining 6 months to qualify the fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_NDB_calib = BetaGeoFitter(penalizer_coef=0.0)\n",
    "\n",
    "# The model is fit only using data up to the end of June 2011\n",
    "beta_NDB_calib.fit(summary_cal_test['frequency_cal'] \\\n",
    "            ,summary_cal_test['recency_cal'] \\\n",
    "            ,summary_cal_test['T_cal'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation of the Beta-Geometric/NBD model\n",
    "calibrated on the first segment of data, and evaluated on the last 6 months"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare for _number of transactions_ :\n",
    "- prediction from the model, over period used for its fit (Calib only, up to June 2011)\n",
    "- the actual data, over the period used for its fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf  = 100\n",
    "for ly in [True,False]:\n",
    "    # https://github.com/CamDavidsonPilon/lifetimes/blob/fb047f7471f07ca4416ddffdfeeca898404cda79/lifetimes/plotting.py#L25\n",
    "    plot_period_transactions(beta_NDB_calib, max_frequency=mf\n",
    "                        ,title=\"Num Transactions (calib dataset)\"\n",
    "                        ,xlabel=\"Number Transactions\"\n",
    "                        ,ylabel=\"Num Customers\"\n",
    "                        ,logy=ly\n",
    "                        ,figsize=(10,6)    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9,6))\n",
    "plot_cumulative_transactions(beta_NDB_calib, df_forRFM\\\n",
    "                             ,'InvoiceDate', 'Customer_ID'\n",
    "                             ,data_span, data_train\n",
    "                             ,title=\"Cumulative Transactions (up to June 2111)\"\n",
    "                             ,xlabel=\"day\"\n",
    "                             ,ylabel=\"Cumulative Transactions\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is it correct we have only 25k transaction, as a result of aggregation of all of those into a single day?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calls plot_cumulative_transactions\n",
    "# which in turns uses\n",
    "#    expected_number_of_purchases_up_to_time()\n",
    "#    method from the fitted model\n",
    "#    to predict the cumulative number of purchases.\n",
    "\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "plot_incremental_transactions(beta_NDB_calib, df_forRFM\n",
    "                             ,'InvoiceDate', 'Customer_ID'\n",
    "                             ,data_span, data_train\n",
    "                             ,freq='D'\n",
    "                             ,title=\"Num Transactions per day\"\n",
    "                             ,xlabel=\"day (since data start)\"\n",
    "                             ,ylabel=\"Transactions/day\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function\n",
    "\n",
    "#             plot_calibration_purchases_vs_holdout_purchases\n",
    "#             https://github.com/CamDavidsonPilon/lifetimes/blob/fb047f7471f07ca4416ddffdfeeca898404cda79/lifetimes/plotting.py#L77\n",
    "\n",
    "# uses internally\n",
    "\n",
    "#             conditional_expected_number_of_purchases_up_to_time\n",
    "#             help(conditional_expected_number_of_purchases_up_to_time)\n",
    "#             https://github.com/CamDavidsonPilon/lifetimes/blob/3e93e805ccb9b4660bd2609a7a9491237029547e/lifetimes/fitters/beta_geo_beta_binom_fitter.py#L179\n",
    "# from the paper:\n",
    "# for the expected number of transactions in a future period of length t for an individual with past observed behavior \n",
    "\n",
    "# to predict into the TEST period using the model,\n",
    "# and plots the average purchase frequency in bins of other variables --<'kind'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the plots have on ABCISSA bins of variables computed in the CALIBRATION period:\n",
    "\n",
    "- k=\"frequency_cal\"               # Purchases in calibration period,\n",
    "- k=\"recency_cal\"                 # Age of customer at last purchase,\n",
    "- k=\"T_cal\"                       # Age of customer at the end of calibration period,\n",
    "- k=\"time_since_last_purchase\"    # Time since user made last purchase\n",
    "\n",
    "## the plots have on ordinate the average number of purchases over the TEST period:\n",
    "- from the real data (the truth for this test)\n",
    "- from the prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ymax=8000\n",
    "for k,xlab in [['frequency_cal','Num purchases (calib.)'],\n",
    "            ['recency_cal','Recency i.e. t_last-t_first (calib.)'],\n",
    "            ['T_cal','Age i.e. t_end_data-t_first (calib.)'],\n",
    "            ['time_since_last_purchase','Tieme since last purchase (calib.)'],]:\n",
    "                plot_calibration_purchases_vs_holdout_purchases(beta_NDB_calib\n",
    "                                                    ,summary_cal_test\n",
    "                                                    ,n=ymax # huge value => always all points \n",
    "                                                    ,figsize=(12,6)\n",
    "                                                    ,title=\"Num Transactions per day\"\n",
    "                                                    ,kind=k )\n",
    "                plt.title('Purchases after June 2011: test data vs prediction')\n",
    "                plt.xlabel(xlab)\n",
    "                plt.ylabel('Average number of purchases')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_cal_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFM_total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following cell can be removed once I'll have put the \n",
    "# prob_alive somewhere else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only the customers who have come back already at least once\n",
    "# RFM_returned = RFM_total[RFM_total['frequency']>0]\n",
    "summary_cal_test['prob_alive'] = \\\n",
    " beta_NDB_calib.conditional_probability_alive(summary_cal_test['frequency_cal'],\n",
    "                                               summary_cal_test['recency_cal'],\n",
    "                                               summary_cal_test['T_cal'])\n",
    "sns.distplot(summary_cal_test['prob_alive'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit the Beta-Geometric/NBD model, FULL Dataset                  (up to end December 2011)\n",
    "- consider the whole dataset\n",
    "- fit the Beta-Geometric/NBD over that period\n",
    "- there''s no test perdiod for this model; we assume that the good accuracy observed when fitting over the limited period supports the validity of the model also over the whole pariod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary of each customer over the whole calibration period\n",
    "# to implement that, set the end of the calibration period 3 days before the end of dataset\n",
    "calib_end='2011-12-07'\n",
    "\n",
    "summary_full_period = calibration_and_holdout_data(df_forRFM,'Customer_ID','InvoiceDate',\n",
    "                                                    calibration_period_end=calib_end,\n",
    "                                                    observation_period_end=obser_end)\n",
    "\n",
    "# The model is fit trougouht the whole dataet\n",
    "beta_NDB_total = BetaGeoFitter(penalizer_coef=0.0)\n",
    "\n",
    "beta_NDB_total.fit(summary_full_period['frequency_cal'] \\\n",
    "            ,summary_full_period['recency_cal'] \\\n",
    "            ,summary_full_period['T_cal'])\n",
    "\n",
    "RFM_total['prob_alive_total'] = \\\n",
    "       beta_NDB_total.conditional_probability_alive(RFM_total['frequency'],\n",
    "                                        RFM_total['recency'],\n",
    "                                        RFM_total['T'])\n",
    "sns.distplot(RFM_total['prob_alive_total'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFM_total.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_full_period.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tit='Alive probability at end of June 2011 (only calib dataset)'\n",
    "summary_cal_test['prob_alive'].plot(kind='hist',bins=20,title=tit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tit='Alive probability at the end of 2011 (full dataset)'\n",
    "RFM_total['prob_alive_total'].plot(kind='hist',bins=20,title=tit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "sns.distplot(RFM_total['prob_alive_total'], hist_kws={'log':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge dataframes such that prob_alive_total and prob_alive are available in the same df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_cal_test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFM_total.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_final=summary_cal_test.join(RFM_total[['prob_alive_total','frequency','recency','T']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(summary_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_final.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_final.plot.scatter(x='prob_alive'\n",
    "                           ,y='prob_alive_total'\n",
    "                           ,figsize=(8,8)\n",
    "                           ,c='DarkBlue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as colors\n",
    "\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "\n",
    "plt.hist2d(summary_final['prob_alive']\n",
    "           , summary_final['prob_alive_total']\n",
    "           , bins=30\n",
    "           #, cmap='Blues'\n",
    "           , cmap='plasma'\n",
    "           , norm=colors.LogNorm()\n",
    "          )\n",
    "plt.xlabel('prob_alive (up to June 2011)')\n",
    "plt.ylabel('prob_aliv_total')\n",
    "\n",
    "cb = plt.colorbar()\n",
    "cb.set_label('Num Customers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPOT CHECKS to make sense of the result...\n",
    "# two customers were dead and \n",
    "# have come back to life shortly before the end of 2011, \n",
    "# see recency \\approx== T, while T_cal very large\n",
    "summary_final.loc[ (summary_final['prob_alive_total'] > 0.95) & \\\n",
    "                  (summary_final['prob_alive'] < 0.07) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dead_up_threshold = 0.10\n",
    "risk_up_threshold = 0.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer to question number 1\n",
    "summary_final.loc[ (summary_final['prob_alive_total'] < dead_up_threshold) & \\\n",
    "                   (summary_final['prob_alive']       > dead_up_threshold) ].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer to question number 2\n",
    "summary_final.loc[ (summary_final['prob_alive_total'] > dead_up_threshold) & \\\n",
    "                   (summary_final['prob_alive_total'] < risk_up_threshold) ].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Have any users churned during the second half of 2011? \n",
    "- I define them as users who have a low prob of being alive at the end of 2011 and had higher prob mid 2011\n",
    "## Are there any users at high risk of churning by the end of 2011?\n",
    "- users who have low live probabiliy at the end of 2011 and didn't mid 2011 \n",
    "\n",
    "- split the users in three populations: dead, alive, high rish\n",
    "- answer to first  question: number of users who were alive|risk in may 2011 and are dead at the end of 2011\n",
    "- answer to second question: number of users who are at high risk at the end of 2011\n",
    "\n",
    "## no need of splitting the customers to train/apply: I just train and verify goodness on variables other than those used. THERE IS NO GROUND TRUTH available in the training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFM_returned['prob_alive'].plot(kind='hist',logy=True,bins=20,title=tit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.distplot(RFM_returned.loc[RFM_returned.prob_alive < 0.3]['prob_alive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFM_returned.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prob_alive_sel = 0.1\n",
    "# RFM_returned.loc[RFM_returned.prob_alive < prob_alive_sel].plot.scatter(x='prob_alive',y='monetary_value')\n",
    "# the customers at risk of churning are not very very valuable\n",
    "# print('there are %d customers with very low probability of being alive (i.e. %s or less)'% (len(RFM_returned.loc[RFM_returned.prob_alive < prob_alive_sel]),prob_alive_sel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.plotting.scatter_matrix(RFM_returned, alpha=0.2,figsize=(15,15))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
