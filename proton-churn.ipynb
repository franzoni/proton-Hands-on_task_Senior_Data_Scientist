{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# GF TODO: check if excel present, otherwise download it\n",
    "# wget http://archive.ics.uci.edu/ml/machine-learning-databases/00502/online_retail_II.xlsx\n",
    "\n",
    "excel_with_path  = '/eos/user/f/franzoni/SWAN_projects/proton/online_retail_II.xlsx'\n",
    "pickle_with_path = '/eos/user/f/franzoni/SWAN_projects/proton/online_retail_II.pkl'\n",
    "csv_with_path = '/eos/user/f/franzoni/SWAN_projects/proton/online_retail_II.csv'\n",
    "\n",
    "df = None\n",
    "\n",
    "if os.path.isfile(pickle_with_path):\n",
    "    print('-> picke already exists, much faster\\n')\n",
    "    df = pd.read_pickle(pickle_with_path)\n",
    "else:\n",
    "    print('-> picke does not exist, go to excel, and create it\\n')\n",
    "    df1 = pd.read_excel (excel_with_path,'Year 2009-2010')\n",
    "    df2 = pd.read_excel (excel_with_path,'Year 2010-2011')\n",
    "    df = pd.concat([df1, df2])\n",
    "    df.to_pickle(pickle_with_path)\n",
    "\n",
    "df['Transaction'] = df.Quantity * df.Price\n",
    "df=df.rename(columns={\"Customer ID\": \"Customer_ID\"})\n",
    "\n",
    "# df.to_csv(csv_with_path, encoding = 'utf-8')\n",
    "\n",
    "# GF TODO: there are nan \n",
    "# GF TODO: there are fields which are 'empty char', e.g. in the customer_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Are there users at high risk of churning by the end of 2011\n",
    "\n",
    "- https://www.kdnuggets.com/2019/05/churn-prediction-machine-learning.html \n",
    "- https://towardsdatascience.com/hands-on-predict-customer-churn-5c2a42806266\n",
    "- https://www.google.com/search?q=dwarf+on+the+shoulders+of+giants&rlz=1C5CHFA_enCH771CH771&sxsrf=ALeKk034vLLmAHn5V1c0QAON4DZuO1GqVA:1582207255390&source=lnms&tbm=isch&sa=X&ved=2ahUKEwiRrayrpeDnAhXCxaYKHZQVCgcQ_AUoAXoECBIQAw&biw=1418&bih=915#imgrc=oj4GwW0aHYfVYM\n",
    "- similar exercise resolved here https://towardsdatascience.com/modeling-customer-churn-for-an-e-commerce-business-with-python-874315e688bf\n",
    "- the repo of the library to be used https://github.com/CamDavidsonPilon/lifetimes\n",
    "- the reference paper in pdf http://brucehardie.com/papers/018/fader_et_al_mksc_05.pdf http://brucehardie.com/papers/bgnbd_2004-04-20.pdf and the journal which has all aspects of a respectable journals \n",
    "- GF TODO: once you've found which customers have churned, make a few hitory plots to prove that indeed they have churned, to show that I am not blindly trusing the package I've downloade\n",
    "\n",
    "\n",
    "\n",
    "- cancellations: fraction of cancellation by nunber of transactions and by proportion of renenue\n",
    "-                correlation to CHURNING ? Correlation to country OR type of good purchased ?\n",
    "\n",
    "\n",
    "- NOT SURE IF USEFUL some stack-overflow like explanations about the Pareto model https://stats.stackexchange.com/questions/251506/is-it-possible-to-understand-pareto-nbd-model-conceptually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --user lifetimes\n",
    "print('pip install worked out fine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from lifetimes.utils import *\n",
    "from lifetimes import BetaGeoFitter,GammaGammaFitter\n",
    "from lifetimes.plotting import plot_probability_alive_matrix, plot_frequency_recency_matrix, plot_period_transactions, plot_cumulative_transactions,plot_incremental_transactions\n",
    "from lifetimes.generate_data import beta_geometric_nbd_model\n",
    "from lifetimes.plotting import plot_calibration_purchases_vs_holdout_purchases, plot_period_transactions,plot_history_alive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(summary_data_from_transaction_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   RFM variables and negative/cancellation transactions\n",
    "- frequency, recency, T\n",
    "- how do I deal with transactions, in producing the RFM models ?\n",
    "- for now just exclude them: cancellations are likely to correlate to churning behaviour, however the frequency of positive transactions is what the pareto model uses and expects to predict churning probabilty\n",
    "- ===> check if there's correlation between churning behaviour and frequency of cancellations ?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FOR the moment I exclude cancellations from the type of transactions to compute RFM variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_forRFM = ['Customer_ID','InvoiceDate','Transaction']\n",
    "df_forRFM = df[columns_forRFM][df['Transaction']>0]\n",
    "#df_forRFM = df[['Customer_ID','InvoiceDate','Transaction']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forRFM.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFM = summary_data_from_transaction_data(df_forRFM,'Customer_ID','InvoiceDate',monetary_value_col='Transaction',freq='D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFM.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RFM info\n",
    "\n",
    "\n",
    "DEFINITIONS\n",
    "- Recency: time between initial purchase and most recent (last) purchase\n",
    "- Frequency: number of repeat purchases made by a customer (total purchases — 1)\n",
    "- Monetary: total spent on purchases\n",
    "\n",
    "FUNCTION summary_data_from_transaction_data\n",
    "- it's a rather clever thing: it AGGREGATES into 1 all transactions taking place at the same DAY!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RFM.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verifiaction of how summary_data_from_transaction_data\n",
    "- for a few Customer_Id randomly picked, Frequency makes sense "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHOULD I RESTRICT MOST OF THE ANALYSIS AND PLOTS TO CUSTOMERS WHO HAVE RETURNED?\n",
    "- if a customer has haver come back, it's probably dead anyway...\n",
    "- and all the values at 0 are somewhat disturbing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_forRFM[df_forRFM['Customer_ID']==12346.0].sort_values(by=['InvoiceDate'],ascending=False)\n",
    "#df_forRFM[df_forRFM['Customer_ID']==12346.0].InvoiceDate.unique()\n",
    "# len(df[df['Customer_ID']==12346.0].InvoiceDate.unique())\n",
    "df_forRFM[df_forRFM['Customer_ID']==12347.0].InvoiceDate.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFM.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tit='Num repeat purchases (tot - 1)'\n",
    "RFM['frequency'].plot(kind='hist',logy=True,title=tit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tit='time bwn initial purchase and most recent (last) purchase'\n",
    "RFM['recency'].plot(kind='hist',logy=True,title=tit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFM.plot.scatter(x='frequency',y='recency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(RFM, alpha=0.2,figsize=(15,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assumptions of the BG/NBD model:\n",
    "- A customer’s relationship has two phases: “alive” for an unobserved period of time, then “dead”\n",
    "- While alive, the number of transactions made by a customer follows a Poisson distribution with transaction rate lambda\n",
    "- Heterogeneity in lambda follows a gamma distribution\n",
    "- After any transaction, a customer dies with probability p; the probability that a customer dies after a number of transactions follows a geometric distribution\n",
    "- p follows a beta distribution\n",
    "- Lambda and p vary independently across customers\n",
    "\n",
    "# seen these references\n",
    "- https://medium.com/data-shopify/how-shopify-merchants-can-measure-retention-c12284bfed6f\n",
    "- http://brucehardie.com/papers/bgnbd_2004-04-20.pdf (of which I have other url's as well)\n",
    "\n",
    "\n",
    "# For follow up\n",
    "\n",
    "- how many customers are there with more than 50/75 purchases ?\n",
    "  Perhaps worth selecting those out and treating them separately ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(BetaGeoFitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_gf = BetaGeoFitter(penalizer_coef=0.0)\n",
    "beta_gf.fit(RFM['frequency'], RFM['recency'], RFM['T']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/CamDavidsonPilon/lifetimes/blob/master/lifetimes/plotting.py#L211\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plot_frequency_recency_matrix(beta_gf);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "plot_probability_alive_matrix(beta_gf);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "plot_period_transactions(beta_gf, max_frequency=250).set_yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "plot_period_transactions(beta_gf, max_frequency=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_star='2009-12-01'\n",
    "calib_end='2011-05-31'\n",
    "obser_end='2011-12-09'\n",
    "\n",
    "from datetime import date\n",
    "d_data_star=date( * map(lambda u:int(u), data_star.split('-') ) )\n",
    "d_calib_end=date( * map(lambda u:int(u), calib_end.split('-') ) )\n",
    "d_obser_end=date( * map(lambda u:int(u), obser_end.split('-') ) )\n",
    "\n",
    "data_span = (d_obser_end - d_data_star).days\n",
    "data_train= (d_calib_end - d_data_star).days\n",
    "print('data cover overall %d days, of which %d are used for training'%(data_span,data_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/CamDavidsonPilon/lifetimes/blob/master/lifetimes/utils.py#L27\n",
    "\n",
    "#  Create a summary of each customer over a calibration and holdout period.\n",
    "#  This function creates a summary of each customer over a calibration and holdout period\n",
    "#  (training and testing, respectively).\n",
    "    \n",
    "summary_cal_holdout = calibration_and_holdout_data(df_forRFM,'Customer_ID','InvoiceDate',\n",
    "                                                    calibration_period_end=calib_end,\n",
    "                                                    observation_period_end=obser_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_cal_holdout.sort_values(by=['frequency_cal'],ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "# non-store online retail between 01/12/2009 and 09/12/2011\n",
    "\n",
    "beta_gf.fit(summary_cal_holdout['frequency_cal'], summary_cal_holdout['recency_cal'], summary_cal_holdout['T_cal'])\n",
    "plot_cumulative_transactions(beta_gf, df_forRFM, 'InvoiceDate', 'Customer_ID', data_span, data_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is it correct we have only 25k transaction, as a result of aggregation of all of those into a single day?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_incremental_transactions(beta_gf, df_forRFM,'InvoiceDate', 'Customer_ID', data_span, data_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k=\"frequency_cal\"               # Purchases in calibration period,\n",
    "#k=\"recency_cal\"                 # Age of customer at last purchase,\n",
    "#k=\"T_cal\"                       # Age of customer at the end of calibration period,\n",
    "#k=\"time_since_last_purchase\"    # Time since user made last purchase\n",
    "\n",
    "for k in ['frequency_cal','recency_cal','T_cal','time_since_last_purchase']:\n",
    "    plot_calibration_purchases_vs_holdout_purchases(beta_gf, summary_cal_holdout,n=250,kind=k);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only the customers who have come back already at least once\n",
    "RFM_returned = RFM[RFM['frequency']>0]\n",
    "RFM_returned['prob_alive'] = beta_gf.conditional_probability_alive(RFM_returned['frequency'],\n",
    "                                                                   RFM_returned['recency'],RFM_returned['T'])\n",
    "sns.distplot(RFM_returned['prob_alive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tit='Alive probability'\n",
    "RFM_returned['prob_alive'].plot(kind='hist',bins=20,title=tit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "sns.distplot(RFM_returned['prob_alive'], hist_kws={'log':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFM_returned['prob_alive'].plot(kind='hist',logy=True,bins=20,title=tit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(RFM_returned.loc[RFM_returned.prob_alive < 0.3]['prob_alive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFM_returned.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_alive_sel = 0.1\n",
    "RFM_returned.loc[RFM_returned.prob_alive < prob_alive_sel].plot.scatter(x='prob_alive',y='monetary_value')\n",
    "# the customers at risk of churning are not very very valuable\n",
    "print('there are %d customers with very low probability of being alive (i.e. %s or less)'% (len(RFM_returned.loc[RFM_returned.prob_alive < prob_alive_sel]),prob_alive_sel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(RFM_returned, alpha=0.2,figsize=(15,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Have any users churned during the second half of 2011? \n",
    "- I define them as users who have a low prob of being alive at the end of 2011 and had higher prob mid 2011\n",
    "## Are there any users at high risk of churning by the end of 2011?\n",
    "- users who have low live probabiliy at the end of 2011 and didn't mid 2011 \n",
    "\n",
    "- split the users in three populations: dead, alive, high rish\n",
    "- answer to first  question: number of users who were alive|risk in may 2011 and are dead at the end of 2011\n",
    "- answer to second question: number of users who are at high risk at the end of 2011\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
